{
 "cells": [
  {
   "cell_type": "raw",
   "id": "12a1fd37",
   "metadata": {},
   "source": [
    "---\n",
    "author: Nipun Batra\n",
    "badges: true\n",
    "categories:\n",
    "- ML\n",
    "date: '2023-12-18'\n",
    "output-file: transcript.html\n",
    "title: YouTube video to transcript using openAI whisper and summary using OLLama\n",
    "toc: true\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90499d25",
   "metadata": {},
   "source": [
    "References\n",
    "1. [Whisper](https://blog.devgenius.io/transcribing-youtube-videos-using-openais-whisper-%EF%B8%8F-%EF%B8%8F-a29d264d6fb1)\n",
    "2. [Langchain and LLama](https://www.youtube.com/watch?v=k_1pOF1mj8k)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c576bebd",
   "metadata": {},
   "source": [
    "### Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f41ca63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8793dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(video_id: str) -> str:\n",
    "    video_url = f'https://www.youtube.com/watch?v={video_id}'\n",
    "    ydl_opts = {\n",
    "        'format': 'm4a/bestaudio/best',\n",
    "        'paths': {'home': 'audio/'},\n",
    "        'outtmpl': {'default': '%(id)s.%(ext)s'},\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'm4a',\n",
    "        }]\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        error_code = ydl.download([video_url])\n",
    "        if error_code != 0:\n",
    "            raise Exception('Failed to download video')\n",
    "\n",
    "    return f'audio/{video_id}.m4a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de0e3377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=CuBzyh4Xmvk\n",
      "[youtube] CuBzyh4Xmvk: Downloading webpage\n",
      "[youtube] CuBzyh4Xmvk: Downloading ios player API JSON\n",
      "[youtube] CuBzyh4Xmvk: Downloading android player API JSON\n",
      "[youtube] CuBzyh4Xmvk: Downloading m3u8 information\n",
      "[info] CuBzyh4Xmvk: Downloading 1 format(s): 140\n",
      "[download] audio/CuBzyh4Xmvk.m4a has already been downloaded\n",
      "[download] 100% of   72.26MiB\n",
      "[ExtractAudio] Not converting audio audio/CuBzyh4Xmvk.m4a; file is already in target format m4a\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'audio/CuBzyh4Xmvk.m4a'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download('CuBzyh4Xmvk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46b2e7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30b1aa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model = whisper.load_model(\"base.en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "933b2158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:00.000 --> 00:05.400]  Please look at the code mentioned above and please sign up on the Google Cloud.\n",
      "[00:05.400 --> 00:08.520]  We've already started making some announcements.\n",
      "[00:08.520 --> 00:14.240]  You will likely end up missing the announcements and you'll have no one else to play with.\n",
      "[00:14.240 --> 00:20.080]  The second quick logistical announcement is that we'll have an extra lecture on Saturday,\n",
      "[00:20.080 --> 00:23.800]  11th Jan at 11am in 1.101.\n",
      "[00:23.800 --> 00:26.240]  So a lot of ones over there.\n",
      "[00:26.240 --> 00:32.000]  And I think one or two people still have conflict, but in the larger, in the larger\n",
      "[00:32.000 --> 00:36.240]  phone we will have almost everyone available, so we'll have to stick with this.\n",
      "[00:36.240 --> 00:43.960]  FAQ and the projects which were earlier shared on Google Docs, I'll give all of you a comment\n",
      "[00:43.960 --> 00:48.960]  access on it so that if you have any questions, queries, things like what should be the,\n",
      "[00:48.960 --> 00:54.160]  what are the maps, what are the main group size you can ask situations if they're already\n",
      "[00:54.160 --> 00:55.160]  not there.\n",
      "[00:55.520 --> 01:00.480]  Also about projects, if you have any questions, like what is the expectation if it's something\n",
      "[01:00.480 --> 01:06.600]  is not mentioned clearly, you can please comment on the Google Doc and we'll get back to you soon.\n",
      "[01:08.560 --> 01:13.160]  Also the video and the slides from the first lecture, in order to actually, actually\n",
      "[01:13.160 --> 01:18.600]  haven't put up on code translate, which is at the entrance, as mentioned in the slide\n",
      "[01:18.600 --> 01:19.600]  above.\n",
      "[01:20.600 --> 01:23.440]  This course website has also been now put on Google Cloud.\n",
      "[01:23.440 --> 01:26.120]  So you can also get to that.\n",
      "[01:28.600 --> 01:33.520]  Before we go forward, we should quickly revise what we started last time.\n",
      "[01:34.280 --> 01:37.800]  And someone tell you what is machine learning based on what we learned last time.\n",
      "[01:38.960 --> 01:40.520]  We looked at a couple of definitions.\n",
      "[01:40.520 --> 01:46.880]  One was from Arthur Sandler, who by the way was the first person to point with the machine learning\n",
      "[01:46.880 --> 01:48.440]  and he did that in 1959.\n",
      "[01:49.440 --> 01:50.720]  So a long long time back.\n",
      "[01:51.720 --> 01:53.720]  Anyone for this machine learning?\n",
      "[02:05.720 --> 02:12.720]  The ability to learn without explicitly being to add others, any definition you want to get?\n",
      "[02:14.720 --> 02:16.720]  There's a more technical definition also.\n",
      "[02:16.720 --> 02:18.720]  But we'll get to that later.\n",
      "[02:18.720 --> 02:23.720]  Let's first start with again this same study, same definition of the learning code.\n",
      "[02:23.720 --> 02:29.720]  It's a period of study and get computers, they are ready to learn without being explicitly programming.\n",
      "[02:29.720 --> 02:33.720]  Okay, anyone tell you what does being explicitly programming here?\n",
      "[02:37.720 --> 02:40.720]  Does a need a machine learning involves low programming?\n",
      "[02:40.720 --> 02:43.720]  So all programming, assignments are just the ways of learning.\n",
      "[02:44.720 --> 02:46.720]  Program itself.\n",
      "[02:46.720 --> 02:47.720]  Program itself.\n",
      "[02:47.720 --> 02:48.720]  Program itself.\n",
      "[02:48.720 --> 02:53.720]  So is it some, some oracle which ends up writing the code?\n",
      "[02:53.720 --> 02:55.720]  It's a whole writing program.\n",
      "[02:56.720 --> 03:02.720]  Of course there's one area, there's something on the computer architecture, so we're not going into that.\n",
      "[03:02.720 --> 03:04.720]  But who I'd say is the machine learning program.\n",
      "[03:05.720 --> 03:07.720]  So today's legislative program.\n",
      "[03:07.720 --> 03:12.720]  What is the exact meaning of adopting the executive program?\n",
      "[03:14.720 --> 03:16.720]  You don't have to replace this.\n",
      "[03:16.720 --> 03:18.720]  Okay, can you explain what does mean?\n",
      "[03:26.720 --> 03:28.720]  But he's on the right track.\n",
      "[03:28.720 --> 03:32.720]  Let's take an example to get this concept even better.\n",
      "[03:34.720 --> 03:36.720]  Can you see these model digits?\n",
      "[03:36.720 --> 03:39.720]  These are digits from 0 to 9.\n",
      "[03:40.720 --> 03:42.720]  And these are from the dataset for in this.\n",
      "[03:42.720 --> 03:45.720]  One of the most popular machine learning datasets.\n",
      "[03:45.720 --> 03:54.720]  Now the first task for all of us is we want to now write a program to recognize the digits.\n",
      "[03:54.720 --> 03:56.720]  And we'll start with code.\n",
      "[03:56.720 --> 04:01.720]  Can someone tell me how they'll recognize if a digit is 4 or not?\n",
      "[04:01.720 --> 04:03.720]  We're looking at these specific rules.\n",
      "[04:03.720 --> 04:07.720]  What does how we add to the code that we need?\n",
      "[04:08.720 --> 04:17.720]  Can we start off as 4 can be quantified as a vertical line, a horizontal line, a vertical line.\n",
      "[04:17.720 --> 04:19.720]  All of them are jointed.\n",
      "[04:19.720 --> 04:24.720]  And then another vertical line going down from the first, from the last vertical line.\n",
      "[04:24.720 --> 04:32.720]  Everything of that is that all that is there to 4 or if there are any models.\n",
      "[04:33.720 --> 04:41.720]  What about the fact that the height of each of the vertical lines need to be very similar?\n",
      "[04:41.720 --> 04:44.720]  Can you write this kind of a rule?\n",
      "[04:44.720 --> 04:50.720]  Or do you see a force where one of the lines is very, very long compared to the other?\n",
      "[04:50.720 --> 04:52.720]  Generally not.\n",
      "[04:52.720 --> 04:56.720]  So you have to report some of these constraints.\n",
      "[04:56.720 --> 04:58.720]  But are you done with it anything more?\n",
      "[04:59.720 --> 05:05.720]  Just look through the example for do you see any force which would violate the difference\n",
      "[05:05.720 --> 05:08.720]  that we've seen it, specify the thought.\n",
      "[05:08.720 --> 05:13.720]  And a thought will look that.\n",
      "[05:13.720 --> 05:16.720]  Okay, the second last is one case.\n",
      "[05:16.720 --> 05:21.720]  What about this one?\n",
      "[05:21.720 --> 05:27.720]  Okay, so each of the vertical lines would be a little smaller.\n",
      "[05:28.720 --> 05:32.720]  In many cases it would be a, and if I were to write, you will not understand it,\n",
      "[05:32.720 --> 05:34.720]  or you will get it.\n",
      "[05:34.720 --> 05:37.720]  So because people write different things.\n",
      "[05:37.720 --> 05:39.720]  So that's another rule that I like.\n",
      "[05:39.720 --> 05:41.720]  Now what do you mean by slides?\n",
      "[05:41.720 --> 05:47.720]  Well, it doesn't mean that it can have an inclination of some 10 degrees, 20 degrees, 30 degrees,\n",
      "[05:47.720 --> 05:49.720]  very excited, none of them.\n",
      "[05:49.720 --> 05:54.720]  So let's say you come up with some number, based on your experience, based on some rules of time.\n",
      "[05:55.720 --> 05:56.720]  But is that all?\n",
      "[05:56.720 --> 06:00.720]  No, some people write 4 with a star, right?\n",
      "[06:00.720 --> 06:07.720]  If you look at this, 1, 2, 3, 4, 5, 4, you have this particular line, we have to join\n",
      "[06:07.720 --> 06:10.720]  like this before piloting the end, right?\n",
      "[06:10.720 --> 06:13.720]  So that is now another rule that I've written.\n",
      "[06:13.720 --> 06:19.720]  You have already come up with 5, 6 of such rules, but that's not all.\n",
      "[06:19.720 --> 06:21.720]  Anything else you can think of?\n",
      "[06:21.720 --> 06:26.720]  That's why we have not talked about the bits of the lines.\n",
      "[06:26.720 --> 06:29.720]  What can we think about that?\n",
      "[06:29.720 --> 06:31.720]  Can we cover some rules?\n",
      "[06:31.720 --> 06:39.720]  Let's say if I'm writing the different mark or if I'm using the pen in different fashion,\n",
      "[06:39.720 --> 06:44.720]  where some of my strokes we fired will be thicker, right?\n",
      "[06:44.720 --> 06:47.720]  Maybe that's another particular rule, right?\n",
      "[06:47.720 --> 06:52.720]  There can be some cases where the width of each of the stroke is a little different.\n",
      "[06:52.720 --> 06:58.720]  You'll have to again capture some of these characteristics, while writing some rules or\n",
      "[06:58.720 --> 07:02.720]  writing a program to recognize 4.\n",
      "[07:02.720 --> 07:11.720]  So what we have done thus far is explicitly programmed to classify order, right?\n",
      "[07:11.720 --> 07:17.720]  So now we understand what is explicit programming, what we know is completely different from this.\n",
      "[07:17.720 --> 07:21.720]  So what if thus far does is we had data?\n",
      "[07:21.720 --> 07:26.720]  Data was these examples that we already had.\n",
      "[07:26.720 --> 07:28.720]  We came up with some rules.\n",
      "[07:28.720 --> 07:33.720]  So these were rules which we as experts suggested.\n",
      "[07:33.720 --> 07:38.720]  And in traditional programming we have some kind of a pattern, some programming line would be right,\n",
      "[07:38.720 --> 07:41.720]  which would recognize all of these.\n",
      "[07:41.720 --> 07:45.720]  Of course, what is presented as a vertical line or horizontal line?\n",
      "[07:45.720 --> 07:48.720]  Are still higher constructs?\n",
      "[07:48.720 --> 07:53.720]  For example, vertical line a program computer does not know what is a vertical line.\n",
      "[07:53.720 --> 07:56.720]  So you have to again boil it down to the computer.\n",
      "[07:56.720 --> 08:00.720]  What do you think the vertical line to the middle means?\n",
      "[08:00.720 --> 08:02.720]  Same.\n",
      "[08:02.720 --> 08:03.720]  Same.\n",
      "[08:03.720 --> 08:04.720]  Same.\n",
      "[08:04.720 --> 08:05.720]  Same.\n",
      "[08:05.720 --> 08:06.720]  Ex-axis.\n",
      "[08:06.720 --> 08:16.720]  So think of it as pixels and all of the pixels would be of similar shape, like vertically going on.\n",
      "[08:16.720 --> 08:21.720]  So we have data rules and traditional programming that gives us the answers.\n",
      "[08:21.720 --> 08:24.720]  Now let's go back to the definition.\n",
      "[08:24.720 --> 08:27.720]  The genome is a period of study as computers.\n",
      "[08:27.720 --> 08:32.720]  We have computers a little bit to learn without being explicit in the program.\n",
      "[08:32.720 --> 08:38.720]  And now to make this particular program, to tell me what, if you have to be able to\n",
      "[08:38.720 --> 08:43.720]  use traditional programming with machine learning, what do we need to do?\n",
      "[08:43.720 --> 08:45.720]  So we are not explicitly programming.\n",
      "[08:45.720 --> 08:46.720]  What changes?\n",
      "[08:46.720 --> 08:51.720]  The Romans are gone there.\n",
      "[08:51.720 --> 09:00.720]  So what we are saying is, I am going to learn with the only explicitly- describe program.\n",
      "[09:00.720 --> 09:02.720]  So we don't need the data.\n",
      "[09:02.720 --> 09:04.720]  We don't need the answers.\n",
      "[09:04.720 --> 09:11.720]  And what we end up with is automatically some function of rules that are being run.\n",
      "[09:11.720 --> 09:14.720]  So this is how traditional programming is.\n",
      "[09:14.720 --> 09:17.720]  The bad line of the system program is very different from machine learning.\n",
      "[09:17.720 --> 09:21.720]  Which I have still done this, done this in a very abstract sense.\n",
      "[09:21.720 --> 09:24.720]  We are slowly going to go deeper from this.\n",
      "[09:24.720 --> 09:30.720]  We also looked at another definition which was a more formal definition of machine learning,\n",
      "[09:30.720 --> 09:33.720]  which was given by Tom, which I like.\n",
      "[09:33.720 --> 09:36.720]  Tom is learning from experience E.\n",
      "[09:36.720 --> 09:43.720]  If you look at the experience E, tasks, P, and a performance measure, P,\n",
      "[09:43.720 --> 09:50.720]  if the performance is improving in the particular task, as to measure by the performance measure.\n",
      "[09:50.720 --> 09:53.720]  And it is improving with experience.\n",
      "[09:54.720 --> 09:58.720]  Let's say in this particular case, the task is what?\n",
      "[09:58.720 --> 10:00.720]  For machine learning.\n",
      "[10:00.720 --> 10:02.720]  To classify digits.\n",
      "[10:02.720 --> 10:05.720]  And what is the input that is typically given?\n",
      "[10:05.720 --> 10:10.720]  You have some, so you said that you have some experience.\n",
      "[10:10.720 --> 10:16.720]  The experience can be you have some images along with the true label.\n",
      "[10:16.720 --> 10:22.720]  So you have elements like this 0 along with that label that is actually 0.\n",
      "[10:22.720 --> 10:26.720]  But you have various different examples.\n",
      "[10:26.720 --> 10:31.720]  And the performance measure P, what do you think is the performance measure P?\n",
      "[10:31.720 --> 10:34.720]  What do you want to optimize on?\n",
      "[10:34.720 --> 10:37.720]  How correctly you are given to classify digits?\n",
      "[10:37.720 --> 10:43.720]  So, could you come up with a more scientific sub-centum for correctness?\n",
      "[10:43.720 --> 10:44.720]  I can see.\n",
      "[10:44.720 --> 10:45.720]  Or similar sub-centum.\n",
      "[10:45.720 --> 10:49.720]  So we look at some of these metrics in today.\n",
      "[10:49.720 --> 10:54.720]  So we will start of place lecture after having revised what is machine learning.\n",
      "[10:54.720 --> 10:58.720]  We will start, we are now starting a company, all of us are starting a company.\n",
      "[10:58.720 --> 11:04.720]  And we want to be the basket of those words or some similar words we saw.\n",
      "[11:04.720 --> 11:05.720]  We want to scale.\n",
      "[11:05.720 --> 11:10.720]  So if you remember one of the keywords which we use a lot in the previous lecture was scaled.\n",
      "[11:10.720 --> 11:15.720]  The problem statement is that you want to predict the quality or condition for computer.\n",
      "[11:15.720 --> 11:17.720]  Given its visual features.\n",
      "[11:18.720 --> 11:25.720]  So we say that our business use cases that growth rates are similar such grocery stores,\n",
      "[11:25.720 --> 11:29.720]  they have some human in the loop who looks at each of the tomatoes.\n",
      "[11:29.720 --> 11:33.720]  And that is in let's say a per minute per tomato.\n",
      "[11:33.720 --> 11:39.720]  So there is a lot of human input involved which is making the whole process look.\n",
      "[11:39.720 --> 11:44.720]  We plan to scale it by using computer vision with your features between the data.\n",
      "[11:44.720 --> 11:47.720]  So what we are going to do is we have now an assembly line.\n",
      "[11:47.720 --> 11:52.720]  You put the tomatoes in the assembly line as the pass of snapshots are taken.\n",
      "[11:52.720 --> 11:56.720]  And you automatically classify whether it is a good tomato or bad tomato.\n",
      "[11:56.720 --> 11:58.720]  And back of the tomatoes are from away.\n",
      "[11:58.720 --> 11:59.720]  Right?\n",
      "[11:59.720 --> 12:00.720]  So you are saying that you are saying that you are saying that you are saying that you are\n",
      "[12:00.720 --> 12:01.720]  using a huge amount of human effort.\n",
      "[12:01.720 --> 12:03.720]  And what are you making your process greater?\n",
      "[12:03.720 --> 12:09.720]  So you are saying that why this process we are able to make the living spirit.\n",
      "[12:09.720 --> 12:10.720]  Right?\n",
      "[12:10.720 --> 12:14.720]  So let's now do one of the machine-like aspect of this problem spirit.\n",
      "[12:14.720 --> 12:20.720]  So if you remember, that is why I just spoken that there is an enerity of data.\n",
      "[12:20.720 --> 12:21.720]  Right?\n",
      "[12:21.720 --> 12:24.720]  There is an enerity of experience and data.\n",
      "[12:24.720 --> 12:28.720]  So let's say that we have some pass data on the quality of tomatoes.\n",
      "[12:28.720 --> 12:34.720]  We have collected thousands of tomatoes and for each of the tomatoes some human expert\n",
      "[12:34.720 --> 12:37.720]  quantified whether it is a good tomato or bad tomato.\n",
      "[12:37.720 --> 12:42.720]  For now it is just any good or bad tomato talk on the scale of all the tomatoes.\n",
      "[12:42.720 --> 12:43.720]  Right?\n",
      "[12:43.720 --> 12:44.720]  There are two classes.\n",
      "[12:44.720 --> 12:45.720]  Two classes.\n",
      "[12:45.720 --> 12:50.720]  What visual features do you think would be useful to characterize the tomato?\n",
      "[12:50.720 --> 12:51.720]  Color?\n",
      "[12:51.720 --> 12:55.720]  Any...so what...what if they would be a good tomato?\n",
      "[12:55.720 --> 12:56.720]  You just...\n",
      "[12:56.720 --> 12:59.720]  Which is more red or more...\n",
      "[12:59.720 --> 13:01.720]  There is something on the shade of red.\n",
      "[13:01.720 --> 13:03.720]  What is the bad tomato?\n",
      "[13:04.720 --> 13:10.720]  Something which is either showing some greenish shade, it may be an anterity.\n",
      "[13:10.720 --> 13:16.720]  Black definitely or if it has some fungus or some other attributes.\n",
      "[13:16.720 --> 13:19.720]  What is the other attributes which make it a little bit of bad?\n",
      "[13:19.720 --> 13:21.720]  It is a shade.\n",
      "[13:21.720 --> 13:22.720]  Is it a shade?\n",
      "[13:22.720 --> 13:24.720]  Is it a shade that's a little bit of a...\n",
      "[13:24.720 --> 13:27.720]  It is a perfect...\n",
      "[13:27.720 --> 13:32.720]  So the...okay, some...some...so the size is another attribute.\n",
      "[13:32.720 --> 13:39.720]  Let's say that we have seen for now that all tomatoes are roughly over and smaller tomatoes\n",
      "[13:39.720 --> 13:42.720]  are bad, very big tomatoes are also bad.\n",
      "[13:42.720 --> 13:47.720]  They are very injected with some growth...growth chemicals.\n",
      "[13:47.720 --> 13:52.720]  Size, whether we have seen, what are the other things you would typically do?\n",
      "[13:52.720 --> 13:53.720]  Fine.\n",
      "[13:53.720 --> 13:56.720]  Yeah, but visual features will not look at the people.\n",
      "[13:56.720 --> 13:58.720]  So you'll have to get some proxy for that.\n",
      "[13:58.720 --> 14:02.720]  So that is another thing which I want all of us to take from the machine learning course.\n",
      "[14:02.720 --> 14:05.720]  What are some of the proxy features we could take from?\n",
      "[14:05.720 --> 14:07.720]  Yes, texture.\n",
      "[14:07.720 --> 14:15.720]  So texture will tell us something about the field of the community, whether it's very rough, very smooth, very light and etc.\n",
      "[14:15.720 --> 14:18.720]  So we looked at these three specific features.\n",
      "[14:18.720 --> 14:20.720]  Would there be others?\n",
      "[14:20.720 --> 14:22.720]  Yes, there might be thousands of other features.\n",
      "[14:22.720 --> 14:25.720]  But for the purposes of this example, it's only these three.\n",
      "[14:26.720 --> 14:31.720]  So we were talking about some vast data or some vast experience that we already have.\n",
      "[14:31.720 --> 14:35.720]  Maybe it exists in a form of potato like this.\n",
      "[14:35.720 --> 14:40.720]  You have some sample of the...you have the color, size, texture, and you have the condition.\n",
      "[14:40.720 --> 14:45.720]  So this condition has been manually and dated written down by a human expert.\n",
      "[14:45.720 --> 14:59.720]  So in this particular table, you think sample number could be a useful attribute of feature to predict the condition?\n",
      "[14:59.720 --> 15:02.720]  How many think yes?\n",
      "[15:02.720 --> 15:05.720]  How many think no?\n",
      "[15:05.720 --> 15:06.720]  Okay.\n",
      "[15:06.720 --> 15:09.720]  Now tell me that it could be a useful feature.\n",
      "[15:09.720 --> 15:11.720]  Now think more and more.\n",
      "[15:11.720 --> 15:15.720]  How could the sample number be useful?\n",
      "[15:15.720 --> 15:23.720]  Sorry.\n",
      "[15:23.720 --> 15:31.720]  No, also the equation is orange.\n",
      "[15:31.720 --> 15:35.720]  If the other is orange, decide the small, the textless group.\n",
      "[15:35.720 --> 15:38.720]  I can roughly say the condition is good.\n",
      "[15:38.720 --> 15:44.720]  So, let us sample the variable to 1, 10 because the parameters will be good or bad.\n",
      "[15:44.720 --> 15:46.720]  Just 1 and 2 are good.\n",
      "[15:46.720 --> 15:49.720]  It sequentially arranged the whole two times first, then.\n",
      "[15:49.720 --> 15:50.720]  Okay.\n",
      "[15:50.720 --> 15:53.720]  We start to define the data.\n",
      "[15:53.720 --> 15:54.720]  Okay.\n",
      "[15:54.720 --> 15:58.720]  But why do you think the sample number could be useful?\n",
      "[15:58.720 --> 16:04.720]  So some of you get it to the point that let's say we sequentially arranged, then we will get something.\n",
      "[16:04.720 --> 16:06.720]  But are we getting something?\n",
      "[16:06.720 --> 16:10.720]  The feature's might change and can't qualify for the same parameter.\n",
      "[16:10.720 --> 16:11.720]  Okay.\n",
      "[16:11.720 --> 16:18.720]  So, he is answering that maybe there is a notion of time associated with samples, very likely they could be.\n",
      "[16:18.720 --> 16:26.720]  And imagine a scenario where there is one specific time of the year, maybe when all of the variables are bad.\n",
      "[16:26.720 --> 16:32.720]  Maybe we will produce a band, maybe the leader, which is bringing the tomatoes was hard, hard gone wrong.\n",
      "[16:32.720 --> 16:33.720]  Something would have happened.\n",
      "[16:33.720 --> 16:37.720]  So, in some very limited cases, the sample number could be used for the feature.\n",
      "[16:37.720 --> 16:46.720]  But it's more likely that it's probably better to include more features, which are capturing the types of things we are looking at.\n",
      "[16:46.720 --> 16:48.720]  For example, what was its vehicle condition?\n",
      "[16:48.720 --> 16:52.720]  How many hours have passed through the way it was made?\n",
      "[16:52.720 --> 16:54.720]  Things like that.\n",
      "[16:54.720 --> 16:57.720]  The sample number might be giving them this up.\n",
      "[16:57.720 --> 17:01.720]  But then this could be the same example as we saw in the last lecture.\n",
      "[17:01.720 --> 17:05.720]  Where more ice beams means more sharp attacks.\n",
      "[17:05.720 --> 17:09.720]  There was some correlation, but there was no correlation.\n",
      "[17:09.720 --> 17:20.720]  So, we have thus discussed that the sample number is likely or unlikely to be a good feature depending on how we model the phone process.\n",
      "[17:20.720 --> 17:23.720]  So, for now, let's ignore the sample number.\n",
      "[17:23.720 --> 17:26.720]  Imagine it does not provide any useful information.\n",
      "[17:26.720 --> 17:30.720]  So, then we have some data table, which looks like the problem.\n",
      "[17:30.720 --> 17:36.720]  We will call this entire table the training set.\n",
      "[17:36.720 --> 17:41.720]  And if you noted, I have labeled them in different columns.\n",
      "[17:41.720 --> 17:44.720]  Anyone wants to tell why you are looking for?\n",
      "[17:44.720 --> 17:49.720]  Okay, and put an output as a very technical term.\n",
      "[17:49.720 --> 17:55.720]  Any other term you could talk about these two paths of a so far way.\n",
      "[17:55.720 --> 17:59.720]  So, this looks like a matrix with real matrix.\n",
      "[17:59.720 --> 18:01.720]  Experience and performance.\n",
      "[18:01.720 --> 18:04.720]  So, the condition is not very common.\n",
      "[18:04.720 --> 18:12.720]  The condition is the annotation or the label or the output assigned to this particular item.\n",
      "[18:12.720 --> 18:14.720]  So, this is one parameter.\n",
      "[18:14.720 --> 18:16.720]  The extra is one parameter.\n",
      "[18:16.720 --> 18:20.720]  It's not very common.\n",
      "[18:20.720 --> 18:22.720]  So, we call these things as features, items, and code\n",
      "[18:22.720 --> 18:23.720]  objects.\n",
      "[18:23.720 --> 18:30.720]  If we go back, let's look at the question which I was asking.\n",
      "[18:30.720 --> 18:33.720]  What features do you think will be used?\n",
      "[18:33.720 --> 18:34.720]  Make sense?\n",
      "[18:34.720 --> 18:36.720]  These things are called features.\n",
      "[18:36.720 --> 18:40.720]  The first three columns in the table are called features.\n",
      "[18:40.720 --> 18:43.720]  They are telling us something useful about the parameter.\n",
      "[18:43.720 --> 18:47.720]  What is the code feature that we are going to use?\n",
      "[18:47.720 --> 18:50.720]  Those also equals by the features that we have over here.\n",
      "[18:50.720 --> 18:52.720]  They are all used anonymously.\n",
      "[18:52.720 --> 18:55.720]  But you can generally use features and attributes.\n",
      "[18:55.720 --> 18:58.720]  Obviates is generally used in some of the features.\n",
      "[18:58.720 --> 19:04.720]  And the output of the condition in this case is called the output of the response variable.\n",
      "[19:04.720 --> 19:09.720]  What is the response once you have observed some features?\n",
      "[19:09.720 --> 19:13.720]  Everyone, here, now.\n",
      "[19:13.720 --> 19:16.720]  We call this a training set.\n",
      "[19:16.720 --> 19:19.720]  And let's, for now, introduce a bit of populism.\n",
      "[19:19.720 --> 19:24.720]  We call this entire matrix as D.\n",
      "[19:24.720 --> 19:28.720]  We call this feature matrix.\n",
      "[19:28.720 --> 19:31.720]  It contains N samples.\n",
      "[19:31.720 --> 19:33.720]  What is N?\n",
      "[19:33.720 --> 19:36.720]  N equals four samples.\n",
      "[19:36.720 --> 19:41.720]  And what is the features in this?\n",
      "[19:41.720 --> 19:45.720]  The number of features is color, size, and texture, which is three features.\n",
      "[19:45.720 --> 19:55.720]  So the matrix X shown in the shader pane is four rows of three columns matrix.\n",
      "[19:55.720 --> 19:59.720]  It contains N samples with a Rp and Mm.\n",
      "[19:59.720 --> 20:06.720]  We can write an individual sample as something like this.\n",
      "[20:06.720 --> 20:10.720]  Like X1, we can write as orange, small, smooth.\n",
      "[20:10.720 --> 20:12.720]  Orange, small, smooth.\n",
      "[20:12.720 --> 20:19.720]  Does anyone want to tell you why X1 is written in a column format where in this particular\n",
      "[20:19.720 --> 20:23.720]  matrix X1 appears in a row?\n",
      "[20:23.720 --> 20:24.720]  Why?\n",
      "[20:24.720 --> 20:26.720]  That is the operation.\n",
      "[20:26.720 --> 20:27.720]  Yes.\n",
      "[20:27.720 --> 20:33.720]  So typically when you consider the features or you consider a particular sample, you consider\n",
      "[20:33.720 --> 20:35.720]  that to be a column vector.\n",
      "[20:35.720 --> 20:40.720]  So this is where now we started to produce a little bit of annotations.\n",
      "[20:40.720 --> 20:46.720]  D sample is a column vector, which is what I'm interested on.\n",
      "[20:46.720 --> 20:48.720]  What is the dimension of this?\n",
      "[20:48.720 --> 20:49.720]  P dot N2.\n",
      "[20:49.720 --> 20:51.720]  P equal to 3 in this case.\n",
      "[20:51.720 --> 20:54.720]  Orange, small, smooth, smooth, smooth, smooth, smooth, smooth.\n",
      "[20:54.720 --> 21:00.720]  That's camera X as XI transpose where I is from month to month.\n",
      "[21:00.720 --> 21:02.720]  This is X1 transpose.\n",
      "[21:02.720 --> 21:11.720]  This is X2 transpose with Rho is, can in fact, the third row is X3 transpose and X4 transpose.\n",
      "[21:11.720 --> 21:12.720]  Right?\n",
      "[21:12.720 --> 21:19.720]  So we are able to now write the matrix X in terms of the individual elements.\n",
      "[21:19.720 --> 21:25.720]  So when we have an output vector Y, now this is going to be Y number one.\n",
      "[21:25.720 --> 21:26.720]  Right?\n",
      "[21:26.720 --> 21:29.720]  Because for now, for the simple example, we have a single output.\n",
      "[21:29.720 --> 21:35.720]  Findle, output variable or response variable, which is going to be Rn.\n",
      "[21:35.720 --> 21:36.720]  Right?\n",
      "[21:36.720 --> 21:38.720]  Because we have any examples.\n",
      "[21:38.720 --> 21:49.720]  Can we thus write this training set D as a set where we have XI transpose from a binary\n",
      "[21:49.720 --> 21:51.720]  and I release from month to month.\n",
      "[21:51.720 --> 21:52.720]  Right?\n",
      "[21:52.720 --> 21:56.720]  So this way we are able to be able to create this entire training set.\n",
      "[21:56.720 --> 22:00.720]  And it can size comma depending on the Ix sample.\n",
      "[22:00.720 --> 22:01.720]  Right?\n",
      "[22:01.720 --> 22:08.720]  So again, XI belongs to, is a BMS to a vector and Y is an Nn to the vector.\n",
      "[22:08.720 --> 22:11.720]  Everyone clear this far?\n",
      "[22:11.720 --> 22:19.720]  Now, the prediction tasks, this is where machine learning lens is usually.\n",
      "[22:19.720 --> 22:20.720]  Right?\n",
      "[22:20.720 --> 22:24.720]  If you only have brain set, there is no access used for it.\n",
      "[22:24.720 --> 22:29.720]  What you wanted to do was for unseen samples of our scheme for these samples, for which\n",
      "[22:29.720 --> 22:33.720]  a human has not yet annotated as a good familiar vector.\n",
      "[22:33.720 --> 22:40.720]  You want them to be fast in the Sunday night and the computer vision approach, which is\n",
      "[22:40.720 --> 22:45.720]  on camera, which is looking at these, the videos you want to tell or the field conditions.\n",
      "[22:45.720 --> 22:46.720]  Right?\n",
      "[22:46.720 --> 22:49.720]  This is a goal clear to everyone.\n",
      "[22:49.720 --> 22:55.720]  So for future unseen tomatoes, for which you don't have a human annotated answer, we want\n",
      "[22:55.720 --> 23:00.720]  to tell whether the condition is better bad and then you want to quality process.\n",
      "[23:00.720 --> 23:07.720]  So we have for these unseen samples that I would draw a line to separate these out.\n",
      "[23:07.720 --> 23:10.720]  For these unseen samples, we still observe the input features.\n",
      "[23:10.720 --> 23:16.720]  We still observe the color, size and texture as given by the computer vision system.\n",
      "[23:16.720 --> 23:17.720]  Right?\n",
      "[23:17.720 --> 23:19.720]  But we don't know the condition.\n",
      "[23:19.720 --> 23:23.720]  That is what we're trying to do.\n",
      "[23:23.720 --> 23:24.720]  Yeah?\n",
      "[23:24.720 --> 23:28.720]  We now break this whole matrix into two subsets.\n",
      "[23:28.720 --> 23:36.720]  The first we've already discussed is the training set, which we said was the matrix D.\n",
      "[23:36.720 --> 23:42.720]  And now we have the test set where we have these specific entries on the condition as unknown,\n",
      "[23:42.720 --> 23:44.720]  which we're trying to estimate today.\n",
      "[23:44.720 --> 23:45.720]  Right?\n",
      "[23:45.720 --> 23:50.720]  So the testing set will be very similar to the training set, but it does not mean the labels\n",
      "[23:50.720 --> 23:52.720]  or the output variable.\n",
      "[23:52.720 --> 23:53.720]  Right?\n",
      "[23:53.720 --> 23:56.720]  Everyone clear the distinction between training and testing?\n",
      "[23:56.720 --> 23:57.720]  Right?\n",
      "[23:57.720 --> 23:58.720]  Okay.\n",
      "[23:58.720 --> 24:05.720]  So given the background that we have thus far, could we now tell what we're trying to do\n",
      "[24:05.720 --> 24:08.720]  from this example in a more succinct fashion?\n",
      "[24:08.720 --> 24:11.720]  What do we hope to do?\n",
      "[24:11.720 --> 24:14.720]  Given the training set, given the test set.\n",
      "[24:14.720 --> 24:19.720]  And we have to have some learning output.\n",
      "[24:19.720 --> 24:24.720]  It's the relating set.\n",
      "[24:24.720 --> 24:30.720]  How do you relate the input to the output?\n",
      "[24:30.720 --> 24:33.720]  I don't need a precise answer, but you can relate.\n",
      "[24:33.720 --> 24:39.720]  You can write the output as some function of the input.\n",
      "[24:39.720 --> 24:42.720]  Thus far, we don't know what kind of function is this.\n",
      "[24:42.720 --> 24:47.720]  And different algorithms we have with different functions, relating the output to the input.\n",
      "[24:47.720 --> 24:54.720]  But we want to be able to predict the output using some functional complex set for now.\n",
      "[24:54.720 --> 24:57.720]  And where do we learn this F from?\n",
      "[24:57.720 --> 24:59.720]  This function F from?\n",
      "[24:59.720 --> 25:01.720]  It's from the training set.\n",
      "[25:01.720 --> 25:05.720]  And where do we apply this function F on?\n",
      "[25:05.720 --> 25:08.720]  On the testing set.\n",
      "[25:08.720 --> 25:15.720]  And given this, for this particular sample, given the inputs red, red, and red, you want\n",
      "[25:15.720 --> 25:16.720]  to predict the condition.\n",
      "[25:16.720 --> 25:17.720]  Right?\n",
      "[25:17.720 --> 25:20.720]  And the general rule would be that we're able to do this accurately otherwise it does not\n",
      "[25:20.720 --> 25:21.720]  make any sense.\n",
      "[25:21.720 --> 25:22.720]  Right?\n",
      "[25:22.720 --> 25:23.720]  Okay.\n",
      "[25:23.720 --> 25:26.720]  Now, a big question.\n",
      "[25:26.720 --> 25:33.720]  Is predicting on the test set enough to say that the model is invalidating?\n",
      "[25:33.720 --> 25:37.720]  Why or why not?\n",
      "[25:37.720 --> 25:44.720]  The test set now is missing out the outliers.\n",
      "[25:44.720 --> 25:45.720]  Okay?\n",
      "[25:45.720 --> 25:49.720]  So, we are saying that it tested by missing out the outliers.\n",
      "[25:49.720 --> 25:56.720]  So, we can add a little more weight rate.\n",
      "[25:56.720 --> 25:57.720]  Okay?\n",
      "[25:57.720 --> 25:58.720]  Okay.\n",
      "[25:58.720 --> 26:01.720]  So, the answer is the answer that he is giving is that there could be some exceptions.\n",
      "[26:01.720 --> 26:03.720]  There could be some outliers.\n",
      "[26:03.720 --> 26:06.720]  But that is getting to the right answer.\n",
      "[26:06.720 --> 26:11.720]  But you need to make a move there.\n",
      "[26:11.720 --> 26:17.720]  The case that, if there is not an annotation, what will be?\n",
      "[26:17.720 --> 26:19.720]  How will you take the accuracy?\n",
      "[26:19.720 --> 26:21.720]  How will we take the accuracy?\n",
      "[26:21.720 --> 26:25.720]  So, we come to the specific question of how we do the check the accuracy.\n",
      "[26:25.720 --> 26:26.720]  Yeah.\n",
      "[26:26.720 --> 26:27.720]  So, we come to that.\n",
      "[26:27.720 --> 26:28.720]  But yes.\n",
      "[26:28.720 --> 26:29.720]  Yes.\n",
      "[26:29.720 --> 26:30.720]  Okay.\n",
      "[26:30.720 --> 26:38.720]  The test set may be a subset of the brain set.\n",
      "[26:38.720 --> 26:41.720]  And your answer was that the test set might have some outliers.\n",
      "[26:41.720 --> 26:44.720]  Both of you are really there.\n",
      "[26:44.720 --> 26:46.720]  Come, take a sum.\n",
      "[26:46.720 --> 26:47.720]  Over-thinking.\n",
      "[26:47.720 --> 26:48.720]  That's good.\n",
      "[26:48.720 --> 26:51.720]  So, we have not thus introduced it on that over-thinking.\n",
      "[26:51.720 --> 26:56.720]  Can you use some simpler numbers we have seen thus far?\n",
      "[26:56.720 --> 27:03.720]  Think of sometimes you might have heard in some probability code.\n",
      "[27:03.720 --> 27:18.720]  So, the ideal thing that we want to do is to be creating well of all possible inputs.\n",
      "[27:18.720 --> 27:19.720]  Right?\n",
      "[27:19.720 --> 27:22.720]  The emphasis on all possible inputs.\n",
      "[27:23.720 --> 27:25.720]  But can you test that?\n",
      "[27:25.720 --> 27:28.720]  Can the test set be all possible inputs?\n",
      "[27:28.720 --> 27:30.720]  Maybe an ideal case.\n",
      "[27:30.720 --> 27:31.720]  Yes.\n",
      "[27:31.720 --> 27:38.720]  But in a more practical case, you know, you will never be able to enumerate all possible test cases.\n",
      "[27:38.720 --> 27:44.720]  And now relating to the answer which some of you have already given.\n",
      "[27:44.720 --> 27:50.720]  So, one way to put it would be that the test set is only a sample from all possible inputs.\n",
      "[27:50.720 --> 27:51.720]  Right?\n",
      "[27:51.720 --> 27:56.720]  So, this now relates to the answer we are talking about outliers.\n",
      "[27:56.720 --> 28:02.720]  You could end up using a test set which is a very, which contains a lot of outliers,\n",
      "[28:02.720 --> 28:07.720]  but does not contain outliers which might be present in all possible inputs.\n",
      "[28:07.720 --> 28:12.720]  And also, it's to your specific answer that the test set would be a part of the brain set\n",
      "[28:12.720 --> 28:16.720]  because it's now only a sample from all possible inputs.\n",
      "[28:17.720 --> 28:23.720]  More generally, we have something where single beta there is some origin or there is some\n",
      "[28:23.720 --> 28:31.720]  generating process which generates the brain data which I am calling there as the empirical\n",
      "[28:31.720 --> 28:34.720]  data sample and I have written a technical along with IID.\n",
      "[28:34.720 --> 28:37.720]  So, this is identity and independence distributed.\n",
      "[28:37.720 --> 28:44.720]  If I would recommend that if you don't know this term again, go back and study some of\n",
      "[28:44.720 --> 28:45.720]  the data.\n",
      "[28:45.720 --> 28:52.720]  So, you can use sample from the hidden or you can write data from the hidden process.\n",
      "[28:52.720 --> 28:59.720]  You are able to get some free data which you learn, you get a model which in a previous\n",
      "[28:59.720 --> 29:02.720]  case was some function S that we will learn it.\n",
      "[29:02.720 --> 29:08.720]  Once you have learned by function you predict using unseen data, you predict another sample.\n",
      "[29:08.720 --> 29:15.720]  So, that sample is again also coming from the same data set, from same underlying distribution\n",
      "[29:15.720 --> 29:19.720]  of the same generating process like both of you.\n",
      "[29:19.720 --> 29:25.720]  So, what now we get is that the brain set and the test set of samples are not from the\n",
      "[29:25.720 --> 29:26.720]  hidden distribution.\n",
      "[29:26.720 --> 29:28.720]  Sometimes it is also known as population.\n",
      "[29:28.720 --> 29:33.720]  We are getting some samples from the population.\n",
      "[29:33.720 --> 29:36.720]  The test set will not contain all the samples.\n",
      "[29:36.720 --> 29:42.720]  In order to say that our model generalizes perfectly, we would have had to see the entire\n",
      "[29:42.720 --> 29:44.720]  population which is never the same.\n",
      "[29:44.720 --> 29:45.720]  Right?\n",
      "[29:45.720 --> 29:53.720]  And you have much more deeper discussion on the differences between the test set and the\n",
      "[29:53.720 --> 29:55.720]  the true population.\n",
      "[29:55.720 --> 30:02.720]  Once we study bias in various ways, the sample topics which hopefully it has come in the\n",
      "[30:02.720 --> 30:03.720]  process of data.\n",
      "[30:03.720 --> 30:04.720]  Everyone clear the lamp?\n",
      "[30:04.720 --> 30:05.720]  Okay.\n",
      "[30:05.720 --> 30:12.720]  So, we have last part seen one particular type of machine learning task where you will\n",
      "[30:12.720 --> 30:17.720]  try to classify whether the object or whether the particular the metals would have had.\n",
      "[30:17.720 --> 30:20.720]  But now look at a very different example.\n",
      "[30:20.720 --> 30:25.720]  We want to predict the energy consumption of IT on the other campus.\n",
      "[30:25.720 --> 30:26.720]  Right?\n",
      "[30:26.720 --> 30:27.720]  So, again the same exercise.\n",
      "[30:27.720 --> 30:31.720]  What factors do you think the energy consumption should depend on?\n",
      "[30:31.720 --> 30:34.720]  Can you quantify that?\n",
      "[30:34.720 --> 30:38.720]  Because failure is again one specific aspect of it.\n",
      "[30:38.720 --> 30:40.720]  Humanity temperature, okay.\n",
      "[30:40.720 --> 30:43.720]  What do you expect the humidity is more?\n",
      "[30:43.720 --> 30:46.720]  Do you think they are doing more of it?\n",
      "[30:46.720 --> 30:47.720]  More?\n",
      "[30:47.720 --> 30:48.720]  Okay.\n",
      "[30:48.720 --> 30:50.720]  One of the temperatures.\n",
      "[30:50.720 --> 30:54.720]  Temperature is more, what energy do you want?\n",
      "[30:54.720 --> 30:55.720]  Five.\n",
      "[30:56.720 --> 30:57.720]  More energy is less.\n",
      "[30:57.720 --> 30:58.720]  Okay.\n",
      "[30:58.720 --> 31:01.720]  One of the other factors in opportunity to depend on.\n",
      "[31:01.720 --> 31:03.720]  Health of the game.\n",
      "[31:03.720 --> 31:04.720]  Health of the game.\n",
      "[31:04.720 --> 31:05.720]  Health of the game.\n",
      "[31:05.720 --> 31:06.720]  Health of the game.\n",
      "[31:06.720 --> 31:07.720]  Okay.\n",
      "[31:07.720 --> 31:08.720]  Okay.\n",
      "[31:08.720 --> 31:11.720]  The number of people, number of occupants and if there are more occupants do you expect\n",
      "[31:11.720 --> 31:12.720]  more energy to happen?\n",
      "[31:12.720 --> 31:13.720]  Yes.\n",
      "[31:13.720 --> 31:14.720]  Generally yes.\n",
      "[31:14.720 --> 31:15.720]  Any other factor?\n",
      "[31:15.720 --> 31:16.720]  No.\n",
      "[31:16.720 --> 31:17.720]  Sorry.\n",
      "[31:17.720 --> 31:18.720]  Okay.\n",
      "[31:19.720 --> 31:20.720]  Okay.\n",
      "[31:20.720 --> 31:27.720]  One of the things I thought there are some other aspects of a campus energy they determine\n",
      "[31:27.720 --> 31:31.720]  when they are going to participate in whether they are in some of the energy.\n",
      "[31:31.720 --> 31:32.720]  Okay.\n",
      "[31:32.720 --> 31:34.720]  What are the assets?\n",
      "[31:34.720 --> 31:36.720]  The standard of force.\n",
      "[31:36.720 --> 31:37.720]  Lab.\n",
      "[31:37.720 --> 31:38.720]  Lab.\n",
      "[31:38.720 --> 31:39.720]  The switch is the standard.\n",
      "[31:39.720 --> 31:44.720]  Lab usage or let's say, machinery or the entire energy.\n",
      "[31:45.720 --> 31:47.720]  The number of computers would be one of the examples.\n",
      "[31:47.720 --> 31:52.720]  The more the computers, the more the servers will generate strength more energy.\n",
      "[31:52.720 --> 31:55.720]  Any other factor we could take off?\n",
      "[31:55.720 --> 32:00.720]  We can make them or tap us in some sense captured by the number of occupants.\n",
      "[32:00.720 --> 32:01.720]  Right?\n",
      "[32:01.720 --> 32:06.720]  So we are not like to explicitly write whether something can or be.\n",
      "[32:06.720 --> 32:08.720]  That is mainly practical.\n",
      "[32:08.720 --> 32:09.720]  Right?\n",
      "[32:09.720 --> 32:13.720]  You think how does that include the population?\n",
      "[32:13.720 --> 32:17.720]  The question is how does weekend or week to relate to the population?\n",
      "[32:17.720 --> 32:22.720]  So I am saying that in some sense, when it is a weekend, you would assume that many of\n",
      "[32:22.720 --> 32:26.720]  the people are let's say, not in the maps.\n",
      "[32:26.720 --> 32:29.720]  Does the population automatically reduce?\n",
      "[32:29.720 --> 32:34.720]  So we say the weekend population would be lesser and the weekend would be less.\n",
      "[32:34.720 --> 32:35.720]  Yes.\n",
      "[32:35.720 --> 32:36.720]  How are you saying?\n",
      "[32:36.720 --> 32:37.720]  We can't be a week.\n",
      "[32:37.720 --> 32:38.720]  We can't be a week.\n",
      "[32:38.720 --> 32:39.720]  We can't be a week.\n",
      "[32:39.720 --> 32:40.720]  We can't be a week.\n",
      "[32:40.720 --> 32:41.720]  We can't be a week.\n",
      "[32:41.720 --> 32:44.720]  So you could always come up with some counter-events.\n",
      "[32:44.720 --> 32:45.720]  Yes.\n",
      "[32:45.720 --> 32:49.720]  But there will be some people who will be leaving, some people who will be coming.\n",
      "[32:49.720 --> 32:51.720]  The day and night, the day and night.\n",
      "[32:51.720 --> 32:52.720]  Day and night.\n",
      "[32:52.720 --> 32:53.720]  We can't be.\n",
      "[32:53.720 --> 32:54.720]  We can't be.\n",
      "[32:54.720 --> 32:55.720]  We can't be.\n",
      "[32:55.720 --> 32:56.720]  Okay.\n",
      "[32:56.720 --> 32:59.720]  So he is saying that the hour of the day is also an important factor.\n",
      "[32:59.720 --> 33:04.720]  How can you gain tellers which are expected more energy consumption?\n",
      "[33:04.720 --> 33:06.720]  So in the end, we are keeping people.\n",
      "[33:06.720 --> 33:08.720]  Day and hour, we can't be able to maintain their thinking.\n",
      "[33:08.720 --> 33:09.720]  Yes.\n",
      "[33:09.720 --> 33:10.720]  We can't assume.\n",
      "[33:10.720 --> 33:12.720]  But we don't have to find.\n",
      "[33:12.720 --> 33:16.720]  For now, let's assume that the numbers are designed and people have to treat it as well.\n",
      "[33:16.720 --> 33:19.720]  So in the night time, we generally see the lot to be low.\n",
      "[33:19.720 --> 33:26.720]  At least in the relative denominator, we are expecting that.\n",
      "[33:26.720 --> 33:31.720]  So more people, generally more energy, higher temperature, generally higher energy.\n",
      "[33:31.720 --> 33:33.720]  Again, it can be constructed in a table like this.\n",
      "[33:34.720 --> 33:35.720]  We have people temperature.\n",
      "[33:35.720 --> 33:38.720]  And just for the purpose of the illustration, that's something that will be factors.\n",
      "[33:38.720 --> 33:42.720]  People in temperature and energy, you know, for our, for our, for our, for our, for our\n",
      "[33:42.720 --> 33:47.720]  people, we could use tools, but, you know, rather, more generally, I'm certain, certain\n",
      "[33:47.720 --> 33:48.720]  areas.\n",
      "[33:48.720 --> 33:51.720]  Now, what is the training center?\n",
      "[33:51.720 --> 33:57.720]  The first three rows and the considered as, as, for any side, because they have the labels,\n",
      "[33:57.720 --> 34:00.720]  also mentioned the output variable or response variable also, maintenance.\n",
      "[34:01.720 --> 34:07.720]  Whereas the set shown below of the last two samples becomes a test set, where the labels\n",
      "[34:07.720 --> 34:11.720]  or the response variable or the target variable of the, of not being mentioned.\n",
      "[34:11.720 --> 34:14.720]  And this is what you're trying to say, right?\n",
      "[34:17.720 --> 34:21.720]  So, we have thus far seen two different kinds of examples.\n",
      "[34:21.720 --> 34:28.720]  Let's try and make a little more abstractly because of two specific classes of problems.\n",
      "[34:29.720 --> 34:32.720]  I'm going to write the first class of problems as classification.\n",
      "[34:33.720 --> 34:38.720]  Here in the output variable of concern is discrete in nature, right?\n",
      "[34:38.720 --> 34:41.720]  There's every one here to understand what is discrete.\n",
      "[34:41.720 --> 34:44.720]  So, discrete means it could be one of few classes.\n",
      "[34:44.720 --> 34:50.720]  It could either be a, so this one of the examples of discrete is binary, whether it's on or off.\n",
      "[34:50.720 --> 34:53.720]  It could also be done to be one of three classes.\n",
      "[34:53.720 --> 34:55.720]  It could also be minor, but it could not.\n",
      "[34:56.720 --> 35:03.720]  Both formerly we say that why I belong to a set from one to C, where we have C classes.\n",
      "[35:03.720 --> 35:09.720]  We have seen one example can I even tell you more examples of classification tasks in my new restaurant.\n",
      "[35:16.720 --> 35:19.720]  Sorry, I have to take any code.\n",
      "[35:19.720 --> 35:21.720]  No, my name is, how many, many?\n",
      "[35:21.720 --> 35:22.720]  Who will then?\n",
      "[35:23.720 --> 35:30.720]  Okay, so the example that is given is, given is, you want to predict who will then, right?\n",
      "[35:30.720 --> 35:34.720]  So, in fact, this reminds me of some very interesting simulation.\n",
      "[35:34.720 --> 35:38.720]  So, I think there's a very nice log by a late silver.\n",
      "[35:38.720 --> 35:45.720]  Those of you who follow sports as well as data, I think should be really looking into that.\n",
      "[35:45.720 --> 35:50.720]  And some of these logs is fact-related to the winners of the football champions,\n",
      "[35:50.720 --> 35:52.720]  the NBA, the NBA, etc.\n",
      "[35:52.720 --> 35:57.720]  You look over the masses, and they have some, they have some confidence with the same\n",
      "[35:57.720 --> 36:00.720]  who is going to win the league, etc.\n",
      "[36:00.720 --> 36:03.720]  Okay, so what are the kinds of results you have?\n",
      "[36:03.720 --> 36:06.720]  Let's focus on how something is written.\n",
      "[36:06.720 --> 36:10.720]  When you also have tie or talk, or particular cases.\n",
      "[36:10.720 --> 36:13.720]  So this is not full class, I guess.\n",
      "[36:13.720 --> 36:18.720]  And what kind of record features do you think you have to get?\n",
      "[36:19.720 --> 36:22.720]  And everyone else can also take this one out.\n",
      "[36:22.720 --> 36:26.720]  So if you want to predict whether India and Sri Lanka are playing that,\n",
      "[36:26.720 --> 36:29.720]  we do something who will win.\n",
      "[36:29.720 --> 36:31.720]  So how will you find that?\n",
      "[36:31.720 --> 36:34.720]  We need to ask you to find out what is going on.\n",
      "[36:34.720 --> 36:37.720]  Playing 11th, playing 11th, fine.\n",
      "[36:37.720 --> 36:40.720]  So you know, by playing 11th, you mean the names?\n",
      "[36:40.720 --> 36:41.720]  You probably not.\n",
      "[36:41.720 --> 36:44.720]  So some notion of the ratings, right?\n",
      "[36:44.720 --> 36:49.720]  All the having said that, only yesterday I read about the very interesting article.\n",
      "[36:49.720 --> 36:54.720]  So there was some research paper which mentioned that some of those really chess playing\n",
      "[36:54.720 --> 36:59.720]  as a play over natural language process, a natural language processing game.\n",
      "[36:59.720 --> 37:03.720]  I'm not looking at the inherent notion of what both means.\n",
      "[37:03.720 --> 37:06.720]  So they just gave it a specific easy code, etc.\n",
      "[37:06.720 --> 37:09.720]  And that will hopefully be a little bit of a comedy level.\n",
      "[37:09.720 --> 37:13.720]  So those of bell words where they might find this very interesting and shocking.\n",
      "[37:13.720 --> 37:19.720]  For cricket, mainly when you are looking at the player ratings, maybe the player age,\n",
      "[37:19.720 --> 37:23.720]  maybe if it does develop something on that start.\n",
      "[37:23.720 --> 37:26.720]  So it's a blast phase.\n",
      "[37:26.720 --> 37:31.720]  Yes, if there are, if there are, oh, this is the way.\n",
      "[37:31.720 --> 37:37.720]  And in fact, if any of you play games like T by Naj, then what you will do is all that.\n",
      "[37:37.720 --> 37:42.720]  They take you to the configuration, and they are able to simulate both series.\n",
      "[37:42.720 --> 37:46.720]  Any other task, documentation, talk you can think of.\n",
      "[37:46.720 --> 37:48.720]  End of.\n",
      "[37:48.720 --> 37:50.720]  End of the scene, okay.\n",
      "[37:50.720 --> 37:52.720]  So, I'm going to ask later on image.\n",
      "[37:52.720 --> 37:57.720]  You want to classify whether it's an image or it's an university process or an all-new thing.\n",
      "[37:57.720 --> 38:00.720]  You have not two output classes.\n",
      "[38:00.720 --> 38:05.720]  And what is the input that you're given?\n",
      "[38:05.720 --> 38:08.720]  So the input would be an image, right?\n",
      "[38:08.720 --> 38:12.720]  So, now the main events of some items are written.\n",
      "[38:12.720 --> 38:16.720]  So, for the previous if you look, the kinds of inputs that we have are putting into the\n",
      "[38:16.720 --> 38:17.720]  matrix.\n",
      "[38:17.720 --> 38:22.720]  So, in sample, most corresponding to only, you know, let's say p readers.\n",
      "[38:22.720 --> 38:26.720]  But now you have a, let's say p cross q matrix.\n",
      "[38:26.720 --> 38:31.720]  So again, you have to figure out how you will put the p cross q matrix into that particular\n",
      "[38:31.720 --> 38:32.720]  scheme.\n",
      "[38:32.720 --> 38:33.720]  Okay.\n",
      "[38:33.720 --> 38:34.720]  Any other example?\n",
      "[38:34.720 --> 38:35.720]  Okay.\n",
      "[38:35.720 --> 38:41.720]  Shouldn't we, will it be, will it be, will it be, will it be, will it be, will it be,\n",
      "[38:41.720 --> 38:44.720]  or, will it be, will it be, will it be, will it be, will it be, will it be, will it be,\n",
      "[38:44.720 --> 38:45.720]  okay.\n",
      "[38:45.720 --> 38:46.720]  Will I get a loan or not?\n",
      "[38:46.720 --> 38:51.720]  Depending on some, some idea, will I pass the machine or reports or not?\n",
      "[38:51.720 --> 38:54.720]  Will I remain away from the, like, end of lecture or not?\n",
      "[38:54.720 --> 38:56.720]  What is the quality of food, good, bad?\n",
      "[38:56.720 --> 38:59.720]  I think I have multiple such classes.\n",
      "[38:59.720 --> 39:04.720]  What grade range in this class?\n",
      "[39:04.720 --> 39:09.720]  A, B, C, E, all of the specific version of grade, this becomes a very interesting topic\n",
      "[39:09.720 --> 39:10.720]  we'll talk about this later.\n",
      "[39:10.720 --> 39:15.720]  There is also some notion of ordering associated with grades.\n",
      "[39:15.720 --> 39:19.720]  Sometimes, quantified, these, they are involved.\n",
      "[39:19.720 --> 39:22.720]  We're not going to be able to back up.\n",
      "[39:22.720 --> 39:27.720]  The second example that we saw fits into something known as regression.\n",
      "[39:27.720 --> 39:31.720]  They are put variables, continuous and nature, right, to a large extent.\n",
      "[39:31.720 --> 39:36.720]  As we can, as we can, so we can write yi is a real number.\n",
      "[39:36.720 --> 39:39.720]  Okay, give me some examples of regression.\n",
      "[39:39.720 --> 39:44.720]  You want to predict a real number.\n",
      "[39:44.720 --> 39:48.720]  The amount of range for the number.\n",
      "[39:48.720 --> 39:50.720]  The amount of range for the number.\n",
      "[39:50.720 --> 39:56.720]  So, that's the, I have to say, that I have to write.\n",
      "[39:56.720 --> 39:59.720]  Stop market, what do you want to predict?\n",
      "[39:59.720 --> 40:02.720]  The price of a particular class.\n",
      "[40:02.720 --> 40:03.720]  Okay, any other example?\n",
      "[40:03.720 --> 40:06.720]  That's the fourth part of the study.\n",
      "[40:06.720 --> 40:07.720]  Okay.\n",
      "[40:07.720 --> 40:08.720]  How many grants of food?\n",
      "[40:08.720 --> 40:13.720]  Okay, how many grants with our teams for, again, very interesting points of rates of them.\n",
      "[40:13.720 --> 40:17.720]  Because again, if you look at some of the sports, people have already come up with some tools of the family.\n",
      "[40:17.720 --> 40:19.720]  You are only three back to the family.\n",
      "[40:19.720 --> 40:27.720]  So, you can get half the 30-year-old to double the promise, verify those tools.\n",
      "[40:27.720 --> 40:34.720]  And before we get into the actual algorithms, I wanted to talk about the performance measure P, which we discussed in the lecture.\n",
      "[40:34.720 --> 40:40.720]  So, we had talked about experience P, task P, and performance measure P.\n",
      "[40:40.720 --> 40:45.720]  I thought it's more important to first understand what different metrics mean.\n",
      "[40:45.720 --> 40:49.720]  And what does it mean that we have done for job and machine learning or not?\n",
      "[40:49.720 --> 40:52.720]  We started with some metrics for class application.\n",
      "[40:52.720 --> 40:56.720]  Let's assume that we had a groundwork wide.\n",
      "[40:56.720 --> 41:00.720]  So, groundwork means the correct labels that we've got.\n",
      "[41:00.720 --> 41:05.720]  Let's say we had a particular set of regulators.\n",
      "[41:05.720 --> 41:08.720]  Or the first meter, we know that it was good.\n",
      "[41:08.720 --> 41:10.720]  Some human and a different.\n",
      "[41:10.720 --> 41:13.720]  Second meter, some unsighted is good and the other three, some unsighted.\n",
      "[41:13.720 --> 41:16.720]  Some export labels are not bad.\n",
      "[41:16.720 --> 41:20.720]  And then there is some algorithm, some function F that we've done.\n",
      "[41:20.720 --> 41:25.720]  That ends up predicting good, good, good, good, good and bad.\n",
      "[41:25.720 --> 41:28.720]  And we call this vector as y-hat.\n",
      "[41:28.720 --> 41:33.720]  So, we'll use this hat a lot in machine learning.\n",
      "[41:33.720 --> 41:36.720]  hat generally signifies an estimate.\n",
      "[41:36.720 --> 41:41.720]  What is your estimate of these status of regulators?\n",
      "[41:41.720 --> 41:44.720]  So, it is y-hat.\n",
      "[41:44.720 --> 41:47.720]  And normal comes from the actual premise set.\n",
      "[41:47.720 --> 41:49.720]  For now, that's the thing we have.\n",
      "[41:49.720 --> 41:52.720]  Some of them have said it comes from the lower the example.\n",
      "[41:52.720 --> 41:55.720]  And the prediction is made by the model.\n",
      "[41:55.720 --> 42:00.720]  So, what are the different metrics we could use to tell the real data in the algorithm?\n",
      "[42:00.720 --> 42:01.720]  What is it?\n",
      "[42:01.720 --> 42:04.720]  You have done a good job in predicting.\n",
      "[42:04.720 --> 42:06.720]  What has been done back?\n",
      "[42:06.720 --> 42:09.720]  It's bad job.\n",
      "[42:09.720 --> 42:12.720]  Okay, how many things you have done in the world?\n",
      "[42:12.720 --> 42:15.720]  Okay, why?\n",
      "[42:15.720 --> 42:17.720]  I think we're in the computer.\n",
      "[42:17.720 --> 42:20.720]  Sorry.\n",
      "[42:20.720 --> 42:24.720]  Depends on the answer.\n",
      "[42:24.720 --> 42:32.720]  The answer is depends on the state of PR, which is a very valid answer because what is fine to say is that you want to get an externalized\n",
      "[42:32.720 --> 42:36.720]  reactivity of the metric that we put up on it.\n",
      "[42:36.720 --> 42:40.720]  Just saying that 80% accurate does not mean that.\n",
      "[42:40.720 --> 42:49.720]  Only if the best report thus far was 60% and 80% accuracy means a lot.\n",
      "[42:49.720 --> 42:59.720]  We first look at a very simple metric on reactivity, which basically tells us that how many you look at the accuracy\n",
      "[42:59.720 --> 43:01.720]  across y-app.\n",
      "[43:01.720 --> 43:07.720]  We look at the predictions across specific rows and lists and how many times is y-app equal to y?\n",
      "[43:07.720 --> 43:10.720]  Divided by the length of y-app or y?\n",
      "[43:10.720 --> 43:11.720]  Right?\n",
      "[43:11.720 --> 43:16.720]  So, how many times have we accurately or correctly predicted the label?\n",
      "[43:16.720 --> 43:22.720]  The accuracy in this case is in terms of proportion is 0.6 in terms of percentage is 60%.\n",
      "[43:22.720 --> 43:23.720]  Right?\n",
      "[43:23.720 --> 43:26.720]  So, it tells you one specific number.\n",
      "[43:26.720 --> 43:28.720]  This is often not enough.\n",
      "[43:28.720 --> 43:32.720]  And there are different cases in which we need to again contextualize.\n",
      "[43:32.720 --> 43:38.720]  Let's look at some specific types of data to motivate the algorithm.\n",
      "[43:38.720 --> 43:42.720]  Let's imagine that we have 1.0.1 samples.\n",
      "[43:42.720 --> 43:46.720]  Any of predicting something which is not a scale of either, which is either a word or a word.\n",
      "[43:46.720 --> 43:54.720]  And we have, and for examples where the ground growth or the actual samples are good, right?\n",
      "[43:54.720 --> 44:02.720]  And one sample where the actual quality is bad, right?\n",
      "[44:02.720 --> 44:08.720]  Now, there could be multiple cases when it's such kind of data-specific.\n",
      "[44:08.720 --> 44:18.720]  For example, in cancer screening, you hope that in most cases, cancer screening, we will have people's health as good.\n",
      "[44:18.720 --> 44:20.720]  They don't have cancer.\n",
      "[44:20.720 --> 44:23.720]  Unfortunately, in a very small case, we can't see.\n",
      "[44:23.720 --> 44:28.720]  But in general, if we look at some data, we will expect it to look something like this.\n",
      "[44:28.720 --> 44:31.720]  We will go to a good growth answer, a good growth answer, a good growth answer.\n",
      "[44:31.720 --> 44:32.720]  Right?\n",
      "[44:32.720 --> 44:35.720]  So, you end up with an imbalance dataset.\n",
      "[44:35.720 --> 44:42.720]  And maybe even if someone is trying to detect problems on imagery or on some speeds,\n",
      "[44:42.720 --> 44:46.720]  most of the time, you should have a growth barrier, a growth barrier, a growth barrier.\n",
      "[44:46.720 --> 44:53.720]  And then, you can't think of a growth barrier.\n",
      "[44:53.720 --> 44:59.720]  Now, let's look at a different metric now.\n",
      "[44:59.720 --> 45:00.720]  It is called precision.\n",
      "[45:00.720 --> 45:06.720]  Let's look at any ground growth, the same paper as picture earlier.\n",
      "[45:06.720 --> 45:12.720]  Now, before looking at this slide, I'm going to tell you what you understand very strong precision.\n",
      "[45:13.720 --> 45:18.720]  In general, in this concern, layman comes.\n",
      "[45:18.720 --> 45:19.720]  Sorry?\n",
      "[45:19.720 --> 45:20.720]  Creatability or anything?\n",
      "[45:20.720 --> 45:22.720]  Creatability, okay.\n",
      "[45:22.720 --> 45:25.720]  What does it mean to be very preferred?\n",
      "[45:25.720 --> 45:28.720]  It means it does.\n",
      "[45:28.720 --> 45:30.720]  It means it does, okay.\n",
      "[45:30.720 --> 45:35.720]  Any other, what does it mean to be preferred?\n",
      "[45:35.720 --> 45:36.720]  Sorry.\n",
      "[45:37.720 --> 45:44.720]  What do you mean by when someone asks, can you speak of the type of thing?\n",
      "[45:44.720 --> 45:48.720]  Or can you write down the term of the type of thing?\n",
      "[45:48.720 --> 45:49.720]  Sorry?\n",
      "[45:49.720 --> 45:56.720]  Do you think of precision in terms of whatever you're writing or whatever you're predicting\n",
      "[45:56.720 --> 45:59.720]  or how to that is meant?\n",
      "[45:59.720 --> 46:01.720]  How true that is?\n",
      "[46:01.720 --> 46:03.720]  Or how much sense that does mean?\n",
      "[46:04.720 --> 46:10.720]  So, more English terms, I mean, layman terms might mean that how much sense does it make\n",
      "[46:10.720 --> 46:12.720]  what you're writing, okay.\n",
      "[46:12.720 --> 46:20.720]  So, now, coming to word, common definition, you look at the times you predicted good, right?\n",
      "[46:20.720 --> 46:22.720]  You predicted how many times?\n",
      "[46:22.720 --> 46:24.720]  One, two, three, and four.\n",
      "[46:24.720 --> 46:27.720]  Out of these, four times you predicted good.\n",
      "[46:27.720 --> 46:29.720]  How many times would you actually go?\n",
      "[46:29.720 --> 46:32.720]  Or was the ground growth also sense?\n",
      "[46:33.720 --> 46:38.720]  So, you predicted good four times, but you will not predict precise in predicting good.\n",
      "[46:38.720 --> 46:45.720]  Your precision was thus far, thus, two or four, which is the only one.\n",
      "[46:45.720 --> 46:46.720]  Right?\n",
      "[46:46.720 --> 46:48.720]  There's everyone in the definition.\n",
      "[46:48.720 --> 46:52.720]  Our precise are you in predicting a particular class.\n",
      "[46:52.720 --> 46:57.720]  Now, precision can be defined in terms of the specific path.\n",
      "[46:57.720 --> 47:00.720]  You could also define precision for math, right?\n",
      "[47:00.720 --> 47:03.720]  What is the precision for math?\n",
      "[47:03.720 --> 47:10.720]  You predicted bad once, but for that specific time it was actually not bad.\n",
      "[47:10.720 --> 47:14.720]  So, you have got real precision in predicting bad.\n",
      "[47:14.720 --> 47:17.720]  So, precision for math could then be zero or one to be zero.\n",
      "[47:17.720 --> 47:20.720]  Precision for math is now two or four, right?\n",
      "[47:20.720 --> 47:26.720]  Or what you need to do is write a fraction of relevant expenses amongst the derivatives.\n",
      "[47:26.720 --> 47:27.720]  Right?\n",
      "[47:28.720 --> 47:36.720]  Now, we come to another metric called the recall, which is of clearly the mirror notion.\n",
      "[47:36.720 --> 47:38.720]  Now, what is the word English?\n",
      "[47:38.720 --> 47:41.720]  What is the English word?\n",
      "[47:41.720 --> 47:44.720]  What is the English word?\n",
      "[47:44.720 --> 47:49.720]  What does it mean that person X has the way to recall?\n",
      "[47:49.720 --> 47:50.720]  Right?\n",
      "[47:50.720 --> 47:51.720]  You want to put it?\n",
      "[47:51.720 --> 47:52.720]  You want to put it?\n",
      "[47:52.720 --> 47:53.720]  You want to put it?\n",
      "[47:53.720 --> 47:54.720]  You want to put it to remember, right?\n",
      "[47:54.720 --> 47:55.720]  Something on that line.\n",
      "[47:55.720 --> 48:01.720]  So, the definition of any call is how many times it was actually good?\n",
      "[48:01.720 --> 48:06.720]  How much of that could be able to recall in a prediction?\n",
      "[48:06.720 --> 48:07.720]  Right?\n",
      "[48:07.720 --> 48:10.720]  So, it was good, fair, fair, and fair.\n",
      "[48:10.720 --> 48:13.720]  I'll be showing it to the apple.\n",
      "[48:13.720 --> 48:17.720]  We have been able to recall two of these three.\n",
      "[48:17.720 --> 48:21.720]  So, three times the definition of the mirror was good.\n",
      "[48:21.720 --> 48:23.720]  We have been able to recall two times.\n",
      "[48:23.720 --> 48:27.720]  That's the call as true at the peak of the 60th interval.\n",
      "[48:27.720 --> 48:28.720]  Right?\n",
      "[48:28.720 --> 48:31.720]  We see a little precision on the clock.\n",
      "[48:31.720 --> 48:34.720]  Everyone will be able to listen.\n",
      "[48:34.720 --> 48:37.720]  Apple again.\n",
      "[48:37.720 --> 48:41.720]  We are trying to predict whether tissues cancer does or not.\n",
      "[48:41.720 --> 48:49.720]  In the ground truth, we see that we have 100 samples out of which only one of them has\n",
      "[48:49.720 --> 48:54.720]  been used for cancer, which is the last sample, which is shown over here.\n",
      "[48:54.720 --> 49:02.720]  And when we are predicting, we predict 99 times that the person or the specific sample is\n",
      "[49:02.720 --> 49:03.720]  not cancerous.\n",
      "[49:03.720 --> 49:07.720]  And one time, which is the first sample, we predict it to be cancerous.\n",
      "[49:07.720 --> 49:08.720]  Right?\n",
      "[49:08.720 --> 49:15.720]  So, now let's try and understand the precision and recall for this set of predictions.\n",
      "[49:15.720 --> 49:18.720]  The accuracy of the system is fairly good.\n",
      "[49:18.720 --> 49:23.720]  Out of the total hundred times, we were accurate 98 times.\n",
      "[49:23.720 --> 49:25.720]  The only two times we are getting wrong is one.\n",
      "[49:25.720 --> 49:30.720]  The time, for the first sample, when we are predicting it to be cancerous, viral.\n",
      "[49:30.720 --> 49:32.720]  In ground truth, it is not cancerous.\n",
      "[49:32.720 --> 49:36.720]  And the other time we are getting it wrong is when we are predicting it to be not cancerous.\n",
      "[49:36.720 --> 49:40.720]  And the ground truth says it is cancerous, which is the last sample.\n",
      "[49:40.720 --> 49:42.720]  So, the accuracy is 98 times.\n",
      "[49:42.720 --> 49:44.720]  You've correctly identified one of the hundred times.\n",
      "[49:44.720 --> 49:47.720]  Now, let's look at the recall.\n",
      "[49:47.720 --> 49:54.960]  Out of the times, the ground truth was true, which is the hundred sample, only a single\n",
      "[49:54.960 --> 49:55.960]  sample.\n",
      "[49:55.960 --> 50:01.720]  Do we predict it to be cancerous in our prediction system?\n",
      "[50:01.720 --> 50:02.720]  No.\n",
      "[50:02.720 --> 50:09.440]  Sort of one time where it was actually cancerous, we are not able to recall that prediction\n",
      "[50:09.440 --> 50:10.440]  correctly.\n",
      "[50:10.440 --> 50:12.720]  That's the recall is 0 or 1, which is 0.\n",
      "[50:12.720 --> 50:15.720]  Let's look at the precision now.\n",
      "[50:15.720 --> 50:22.960]  Out of the one time, which we are predicting the tissue to be cancerous, was it actually\n",
      "[50:22.960 --> 50:23.960]  cancerous?\n",
      "[50:23.960 --> 50:24.960]  No.\n",
      "[50:24.960 --> 50:26.480]  In the ground truth, it was not cancerous.\n",
      "[50:26.480 --> 50:29.920]  Thus, the precision is 0 over 1, which is 0.\n",
      "[50:29.920 --> 50:39.000]  There is another way to look at the previous example, which is known as confusion matrix.\n",
      "[50:39.000 --> 50:42.200]  We have four entries in this confusion matrix.\n",
      "[50:42.200 --> 50:47.080]  The ground truth could be either yes or no, which is cancerous or not cancerous.\n",
      "[50:47.080 --> 50:53.160]  And similarly, we could predict to be either cancerous or not cancerous.\n",
      "[50:53.160 --> 51:01.800]  We saw previously that out of the 90, out of the hundred instances, 98 times when the\n",
      "[51:01.800 --> 51:07.120]  ground truth was not cancerous, we were also able to predict it as not cancerous.\n",
      "[51:07.120 --> 51:12.160]  Thus, the entry corresponding to predicted equal to no and ground truth equal to no\n",
      "[51:12.160 --> 51:15.280]  is 98.\n",
      "[51:15.280 --> 51:20.960]  For one instance, if you look at the first sample, the prediction is yes, but the ground\n",
      "[51:20.960 --> 51:22.800]  truth was no.\n",
      "[51:22.800 --> 51:28.320]  So the prediction is yes, the ground truth is no, which is the first row and the second\n",
      "[51:28.320 --> 51:29.320]  column.\n",
      "[51:29.320 --> 51:34.800]  And then if we go back, we thought there was one sample where the ground truth was yes,\n",
      "[51:34.800 --> 51:36.760]  but the prediction was no.\n",
      "[51:36.760 --> 51:41.240]  So one sample where the ground truth was yes, the prediction was no, which is the first\n",
      "[51:41.240 --> 51:44.520]  column and the second row.\n",
      "[51:44.520 --> 51:50.680]  Now can you think about precision and recall in terms of these quantities or in terms of\n",
      "[51:50.680 --> 51:53.040]  the confusion matrix?\n",
      "[51:53.040 --> 51:59.920]  But before we do that, let us make the confusion matrix more generalizable.\n",
      "[51:59.920 --> 52:02.360]  So we now have four quantities.\n",
      "[52:02.360 --> 52:06.960]  We have four numbers which are written as true positive, false positive, false negative\n",
      "[52:06.960 --> 52:07.960]  and true negative.\n",
      "[52:08.960 --> 52:12.360]  Let us try and understand how do we remember these four names.\n",
      "[52:12.360 --> 52:15.440]  Let us look at the first true positive.\n",
      "[52:15.440 --> 52:20.560]  The ground truth was positive and we are predicting it to be true.\n",
      "[52:20.560 --> 52:23.000]  We are truly predicting it to be positive.\n",
      "[52:23.000 --> 52:25.280]  Thus it is truly predicted as positive.\n",
      "[52:25.280 --> 52:26.920]  That is true positive.\n",
      "[52:26.920 --> 52:31.800]  The first one in the second column is false positive.\n",
      "[52:31.800 --> 52:38.180]  So the ground truth was not positive, but we are falsely predicting it to be positive.\n",
      "[52:38.180 --> 52:40.360]  Thus false positive.\n",
      "[52:40.360 --> 52:42.920]  The third element is false negatives.\n",
      "[52:42.920 --> 52:49.360]  The ground truth was yes or positive, but we are falsely predicting it to be negative.\n",
      "[52:49.360 --> 52:54.320]  So it is a false negative and the last entry is a true negative.\n",
      "[52:54.320 --> 52:57.640]  The ground truth was a negative and the prediction was also negative.\n",
      "[52:57.640 --> 53:03.360]  Thus you truly predicting it to be negative.\n",
      "[53:03.360 --> 53:08.720]  Now let us come back to the definitions of recall and precision and try to write them\n",
      "[53:08.720 --> 53:15.560]  in terms of the four quantities from the confusion matrix that we have just seen.\n",
      "[53:15.560 --> 53:21.680]  So when we talk about precision, we spoke about how correct we are when we predict it\n",
      "[53:21.680 --> 53:23.680]  to be the positive class.\n",
      "[53:23.680 --> 53:26.760]  Let us say yes in this case.\n",
      "[53:26.760 --> 53:29.960]  What are the total number of times we predicted it to yes?\n",
      "[53:29.960 --> 53:30.960]  How accurate we were?\n",
      "[53:30.960 --> 53:36.240]  So the first row corresponds to the total number of times we predicting it to yes.\n",
      "[53:36.240 --> 53:40.600]  This becomes a denominator which is true positive plus false positive.\n",
      "[53:40.600 --> 53:44.680]  And the portion where we correct is the true positive.\n",
      "[53:44.680 --> 53:50.000]  Where we were predicting it to be positive or yes when it is actually yes.\n",
      "[53:50.000 --> 53:53.440]  Thus the numerator becomes true positive.\n",
      "[53:53.440 --> 54:00.360]  Thus the precision is given by true positive over true positive plus false positive.\n",
      "[54:00.360 --> 54:04.080]  Similarly let us think about recall.\n",
      "[54:04.080 --> 54:11.720]  For recall we said that out of the instances which were true in the ground truth, how many\n",
      "[54:11.720 --> 54:13.800]  are we able to recall?\n",
      "[54:13.800 --> 54:20.000]  So thus the instances which were true in the ground truth becomes a denominator which is\n",
      "[54:20.080 --> 54:26.320]  the first column of this matrix which corresponds to true positive plus false negative.\n",
      "[54:26.320 --> 54:30.720]  And the fraction which is identified correctly is the true positive.\n",
      "[54:30.720 --> 54:34.320]  Or what we are able to recall is the true positive.\n",
      "[54:34.320 --> 54:42.480]  Thus the recall becomes true positive over true positive plus false negative.\n",
      "[54:42.480 --> 54:48.000]  We have another metric called the F score which combines the precision and recall in\n",
      "[54:48.000 --> 54:49.000]  the following way.\n",
      "[54:49.000 --> 54:54.840]  So the definition is given by the formula is given by twice precision times recall divided\n",
      "[54:54.840 --> 54:59.880]  by a precision plus recall it is sometimes useful to give a single number instead of\n",
      "[54:59.880 --> 55:05.280]  giving a precision and a recall.\n",
      "[55:05.280 --> 55:09.440]  There is another interesting metric called Matthew's correlation coefficient.\n",
      "[55:09.440 --> 55:14.560]  The formula looks fairly complicated at this point of time if you see.\n",
      "[55:14.560 --> 55:20.560]  But there is one particular reason why this coefficient is very useful.\n",
      "[55:20.560 --> 55:27.760]  And to see that specific reason let us try to work out a simple example now.\n",
      "[55:27.760 --> 55:32.680]  For the data that you have given below where the ground truth positive and predicted positive\n",
      "[55:32.680 --> 55:35.800]  is the largest number 90.\n",
      "[55:35.800 --> 55:40.440]  And the other three entries are also in the filled in the matrix and fusion matrix.\n",
      "[55:40.440 --> 55:45.400]  And calculate the precision recall as score and Matthew's coefficient.\n",
      "[55:45.400 --> 55:51.440]  Okay, let us talk about precision for now.\n",
      "[55:51.440 --> 55:55.560]  The precision is out of the times you are predicting it to be positive how correct you\n",
      "[55:55.560 --> 55:56.800]  are.\n",
      "[55:56.800 --> 56:02.440]  For that we look at the row corresponding to predictive positive that becomes a denominator.\n",
      "[56:02.440 --> 56:06.440]  Thus the total entries are 90 plus 4 which is 94.\n",
      "[56:06.440 --> 56:10.400]  And how many of them are we correctly identifying that is 90.\n",
      "[56:10.400 --> 56:15.560]  Thus precision becomes 90 over 94.\n",
      "[56:15.560 --> 56:21.460]  Similarly if you look for recall out of the entries which were positive in the ground\n",
      "[56:21.460 --> 56:27.120]  truth that becomes the first column that is 90 plus 1 which is 91 entries.\n",
      "[56:27.120 --> 56:30.480]  How many are we able to recall correctly that is 90.\n",
      "[56:30.480 --> 56:34.480]  Thus recall becomes 90 over 91.\n",
      "[56:34.480 --> 56:40.360]  And we can calculate the score by twice precision times recall divided by precision plus recall.\n",
      "[56:40.360 --> 56:46.720]  Now all of these numbers are giving an indication that we have done a very good job by identification\n",
      "[56:46.720 --> 56:48.640]  or a prediction.\n",
      "[56:48.640 --> 56:51.080]  But does this seem to be a problem?\n",
      "[56:51.080 --> 57:00.360]  Yes, so the problem is that this was a very very easy problem for classification because\n",
      "[57:00.360 --> 57:03.680]  most of the instances were positive in the ground truth.\n",
      "[57:03.680 --> 57:08.120]  So if you predicted everything to be positive you will have a fairly high precision and\n",
      "[57:08.200 --> 57:11.720]  recall and accuracy and F score.\n",
      "[57:11.720 --> 57:15.240]  But the Matthew's coefficient comes out to be fairly low.\n",
      "[57:15.240 --> 57:20.480]  What this is telling us is that you are not doing a substantially good job identifying\n",
      "[57:20.480 --> 57:25.360]  or at predicting in such a case because the problem itself was fairly simple.\n",
      "[57:25.360 --> 57:31.000]  So this is where we need to take all the metrics and all the results with the salt of grain.\n",
      "[57:31.000 --> 57:38.080]  Because it is important to look at how easy or difficult it was when you could have predicted\n",
      "[57:38.080 --> 57:40.280]  the most occurring class.\n",
      "[57:40.280 --> 57:45.080]  I, Grandfords, has a vector of grain numbers and the prediction will also be a vector of\n",
      "[57:45.080 --> 57:46.080]  grain number.\n",
      "[57:46.080 --> 57:47.080]  Right?\n",
      "[57:47.080 --> 57:52.080]  The first metric between look at is called the main squared error.\n",
      "[57:52.080 --> 57:57.080]  The way to remember this is to compose the three terms, main squared error and then computed\n",
      "[57:57.080 --> 57:58.080]  in reverse back.\n",
      "[57:58.080 --> 58:06.080]  You first compute the error which is y i hat minus y i.\n",
      "[58:06.080 --> 58:07.080]  Right?\n",
      "[58:07.080 --> 58:10.080]  Predicted minus ground for the i-n sample.\n",
      "[58:10.080 --> 58:15.080]  You have computed the error and also shown the corresponding error.\n",
      "[58:15.080 --> 58:23.080]  And see the different colors now.\n",
      "[58:24.080 --> 58:29.080]  So y i minus y, y i minus y i tells you the error term.\n",
      "[58:29.080 --> 58:31.080]  Then you have a squared term.\n",
      "[58:31.080 --> 58:34.080]  So y i minus y i hat minus y i.\n",
      "[58:34.080 --> 58:36.080]  For example, you squared the error.\n",
      "[58:36.080 --> 58:37.080]  Right?\n",
      "[58:37.080 --> 58:39.080]  And then you find it in the main over it.\n",
      "[58:39.080 --> 58:41.080]  Which is the main over the n sample.\n",
      "[58:41.080 --> 58:42.080]  Right?\n",
      "[58:42.080 --> 58:43.080]  Main squared error.\n",
      "[58:43.080 --> 58:46.080]  First the error squared, it can be.\n",
      "[58:46.080 --> 58:51.080]  And a term is generally then used in the written or recorded root mean squared error which is\n",
      "[58:51.080 --> 58:54.080]  the root of the mean squared error.\n",
      "[58:54.080 --> 58:55.080]  Right?\n",
      "[58:55.080 --> 59:00.080]  There are another very similar metric called the main absolute error.\n",
      "[59:00.080 --> 59:05.080]  Again, it starts with the inside the first calculate the error y i hat and y i.\n",
      "[59:05.080 --> 59:08.080]  If the absolute value is up it, then they do.\n",
      "[59:08.080 --> 59:09.080]  Right?\n",
      "[59:09.080 --> 59:12.080]  We can also come with a metric called the main error.\n",
      "[59:12.080 --> 59:15.080]  We just first compute the error and take a mean by it.\n",
      "[59:15.080 --> 59:17.080]  Why is that a parameter?\n",
      "[59:17.080 --> 59:18.080]  Use it as a metric.\n",
      "[59:19.080 --> 59:21.080]  They would cancel each other out.\n",
      "[59:21.080 --> 59:26.080]  If you could have a prediction imagine the number of 0 0 0 0 0.\n",
      "[59:26.080 --> 59:29.080]  Or you have four examples of 0.\n",
      "[59:29.080 --> 59:32.080]  And I have predicted as plus and minus and plus and minus n.\n",
      "[59:32.080 --> 59:34.080]  What is the mean error in the skits?\n",
      "[59:34.080 --> 59:36.080]  It's mean error is 0.\n",
      "[59:36.080 --> 59:38.080]  What is the mean absolute error?\n",
      "[59:38.080 --> 59:40.080]  What's the mean?\n",
      "[59:40.080 --> 59:41.080]  What's the mean?\n",
      "[59:41.080 --> 59:42.080]  What's the mean error?\n",
      "[59:42.080 --> 59:43.080]  Okay?\n",
      "[59:43.080 --> 59:45.080]  And I will see you to go over to the main.\n",
      "[59:45.080 --> 59:46.080]  And then.\n",
      "[59:47.080 --> 59:50.080]  So this is why it's also what it's known what the metrics are trying to.\n",
      "[59:50.080 --> 59:54.080]  You're trying to optimize them.\n",
      "[59:54.080 --> 01:00:00.080]  And even tell me why you might want to use mean squared error or mean absolute error times.\n",
      "[01:00:00.080 --> 01:00:01.080]  Right?\n",
      "[01:00:01.080 --> 01:00:02.080]  The expensive.\n",
      "[01:00:02.080 --> 01:00:05.080]  So maybe something more trivial.\n",
      "[01:00:05.080 --> 01:00:06.080]  Sorry.\n",
      "[01:00:06.080 --> 01:00:07.080]  No.\n",
      "[01:00:07.080 --> 01:00:08.080]  No.\n",
      "[01:00:08.080 --> 01:00:09.080]  No.\n",
      "[01:00:09.080 --> 01:00:10.080]  No.\n",
      "[01:00:10.080 --> 01:00:11.080]  No.\n",
      "[01:00:11.080 --> 01:00:12.080]  No.\n",
      "[01:00:12.080 --> 01:00:13.080]  No.\n",
      "[01:00:13.080 --> 01:00:14.080]  No.\n",
      "[01:00:15.080 --> 01:00:17.080]  And that's the mean absolute sentence.\n",
      "[01:00:17.080 --> 01:00:18.080]  Okay.\n",
      "[01:00:18.080 --> 01:00:21.080]  So can you tell me how does the RMSC?\n",
      "[01:00:21.080 --> 01:00:30.080]  So if y i hat is very far away from y i, which is going to produce more than.\n",
      "[01:00:30.080 --> 01:00:39.080]  So can you say that squared errors tend to penalize bad predictions much more than the regular.\n",
      "[01:00:44.080 --> 01:00:54.080]  Now, let us quickly get into the first algorithm for today. We are going to talk about decision\n",
      "[01:00:54.080 --> 01:01:08.080]  trees. We are in now solving a classification problem using a course evaluation for distribution\n",
      "[01:01:08.080 --> 01:01:15.080]  trees. We have some training data where we have different days from D1 to D4E. Should the\n",
      "[01:01:15.080 --> 01:01:20.080]  day be included at an accurate or not? That is the other time, right? So no longer is that\n",
      "[01:01:20.080 --> 01:01:24.080]  the answer now. We have the output of whether it is sunny or hot at the central temperature\n",
      "[01:01:24.080 --> 01:01:29.080]  or hot at the top of the humidity and the wind. And whether or not we play it. Right? So now\n",
      "[01:01:29.080 --> 01:01:34.080]  you are trying to predict whether I should or learn the function between data and some\n",
      "[01:01:34.080 --> 01:01:41.080]  of the attributes of the function. Right? These are, as we described in the computer,\n",
      "[01:01:41.080 --> 01:01:47.080]  uh, COVID attributes and the output variable. And this is practical equation because the\n",
      "[01:01:47.080 --> 01:01:54.080]  output variable of the function is just great. In this case, it is only a yes or no. Right?\n",
      "[01:01:54.080 --> 01:02:00.080]  And because I have also used decision trees to follow regression here where we try to\n",
      "[01:02:00.080 --> 01:02:05.080]  predict the house price given these two I put with any data. Right? Let's get back to the\n",
      "[01:02:05.080 --> 01:02:12.080]  example which we were discussing. I'm not going to talk about this for now. So one of the\n",
      "[01:02:12.080 --> 01:02:18.080]  reasons why I am about to put in different trees is started. We remember last time we\n",
      "[01:02:18.080 --> 01:02:23.080]  were discussing things like over and test block matches. Imagine if we are very very\n",
      "[01:02:23.080 --> 01:02:28.080]  complicated, machinaling the bottom. That says some new, red, or red, or red, or red. There\n",
      "[01:02:28.080 --> 01:02:33.080]  is often a very, there is often a case of very hard to do, red, or red, or red for\n",
      "[01:02:33.080 --> 01:02:39.080]  these models. It's not really to understand what the model is done. But the shift\n",
      "[01:02:39.080 --> 01:02:46.080]  rays being one of the very simple models are very, very suited for such cases. Where you\n",
      "[01:02:46.080 --> 01:02:52.080]  want the model to be able to. Now imagine if you are good for a doctor. Right? You want\n",
      "[01:02:52.080 --> 01:02:56.080]  to build an application, you want to build a machine learning, let's say, answer screen\n",
      "[01:02:56.080 --> 01:03:02.080]  application for a doctor. When the doctor tries that, I applied, signoid and relu and\n",
      "[01:03:02.080 --> 01:03:08.080]  some, uh, phalancy maps and I have done some attention and 20 other technical terms\n",
      "[01:03:08.080 --> 01:03:13.080]  in the event it works. Or they will say that I have some of this set of rules. If the\n",
      "[01:03:13.080 --> 01:03:20.080]  patient has, you know, decaying cell function and patient has some swelling etc. Then the\n",
      "[01:03:20.080 --> 01:03:24.080]  patient is like you have to ask them, which of them do you think is doctorically? Uh,\n",
      "[01:03:24.080 --> 01:03:28.080]  there is, let me share it. Obviously, the second one, right? Because that is\n",
      "[01:03:28.080 --> 01:03:34.080]  good and you are pretty. This involves a lot of many of our things. Right? If we go back\n",
      "[01:03:34.080 --> 01:03:39.080]  to this example and if you think in your life, if you were to play tennis, what is this\n",
      "[01:03:39.080 --> 01:03:45.080]  set of iron? You are very pretty. One, iron, you can just talk to each other\n",
      "[01:03:45.080 --> 01:03:56.080]  and then of course, you can play it at someone's play. You cannot play it at the moment. Uh, but then of course, you not want to play it at this very hot. You not want to play it at the very human.\n",
      "[01:03:56.080 --> 01:04:03.080]  Looking at this, playing it up for a specific example, can you tell me some, some of the\n",
      "[01:04:03.080 --> 01:04:05.080]  times you will definitely play an operator.\n",
      "[01:04:05.080 --> 01:04:15.080]  As an operator, the overcast has always been. I mean, tell you something like that.\n",
      "[01:04:15.080 --> 01:04:34.080]  Okay, in fact, when it is overcast, you see, you are not here always playing. Right? Just look at these specific goals. So, can you come up with these and pull kind of rules? If it is overcast, I am a definitely play. If it is not overcast, it is semi.\n",
      "[01:04:34.080 --> 01:04:40.080]  And let's say that the temperature is mild and added also. Right? So, this is how we\n",
      "[01:04:40.080 --> 01:04:45.080]  find where to create a structure, which is the idea of output of the decision tree.\n",
      "[01:04:45.080 --> 01:04:51.080]  So, this is what we both learned from a machine learning, as well as what we call it,\n",
      "[01:04:51.080 --> 01:05:01.080]  because this is our tree. You can see you have some rules, you have some branches and you have, you have some leaves. These are also telling what is different.\n",
      "[01:05:01.080 --> 01:05:16.080]  And we saw previously, if it is overcast, globalized variables. If it is, if the outlook is overcast, you play a DNS. But if the outlook is semi, and the humidity is lower in silky.\n",
      "[01:05:16.080 --> 01:05:22.080]  It is only when the humidity is high and the outlook is sunny, I will not only play.\n",
      "[01:05:23.080 --> 01:05:31.080]  Similarly, if the outlook is raining, but the wind is weak and still humid. But if the wind is strong, maybe we will not be able to play.\n",
      "[01:05:31.080 --> 01:05:38.080]  Or this specific example. Right? So, this is what we hope to learn using a algorithm.\n",
      "[01:05:38.080 --> 01:05:48.080]  We have data, we have, we have some performance measures, what the accuracy fields are and we have experience coming from this data.\n",
      "[01:05:49.080 --> 01:05:59.080]  So, interestingly, what is an optimum decision tree that we can learn? So, what is the optimum tree that we can learn? We have learned one such tree.\n",
      "[01:05:59.080 --> 01:06:08.080]  Could you have learned many such trees? Could you have learned tree where the path is up here, the path is up here is below.\n",
      "[01:06:08.080 --> 01:06:13.080]  So, there is a very old paper. Does everyone recognize the same?\n",
      "[01:06:13.080 --> 01:06:17.080]  Rather, M is the best. Where have you started?\n",
      "[01:06:17.080 --> 01:06:26.080]  C.L.R.S. which is your algorithm's book is by the same author of this clause, published in the 19th century.\n",
      "[01:06:26.080 --> 01:06:31.080]  Where the measure of constructing optimum binding decision tree is NP-comfy.\n",
      "[01:06:31.080 --> 01:06:39.080]  Which means that we can have so many different trees, we do not, it is not relevant to tell which is the best decision we have to learn.\n",
      "[01:06:40.080 --> 01:06:43.080]  In such cases, what do you typically do?\n",
      "[01:06:54.080 --> 01:07:00.080]  So, my answer is a concept of all the possible trees, but that operation is LLE-comfy.\n",
      "[01:07:00.080 --> 01:07:04.080]  For our purpose, let us say that cannot be done.\n",
      "[01:07:04.080 --> 01:07:07.080]  Or it cannot be done perfectly.\n",
      "[01:07:07.080 --> 01:07:10.080]  So, we need some of the solution to tell which is the good tree.\n",
      "[01:07:10.080 --> 01:07:13.080]  That is the objective to end up creating good trees.\n",
      "[01:07:13.080 --> 01:07:18.080]  And which is good in this area to classify or it is able to predict after it.\n",
      "[01:07:18.080 --> 01:07:21.080]  But we cannot enumerate all possible trees.\n",
      "[01:07:21.080 --> 01:07:26.080]  So, we end up using something called the greedy algorithm.\n",
      "[01:07:26.080 --> 01:07:28.080]  So, greedy means literally greedy.\n",
      "[01:07:29.080 --> 01:07:37.080]  And the intuition is that at each level of the tree, we choose an attribute that gives us the biggest estimated performance gain.\n",
      "[01:07:37.080 --> 01:07:40.080]  We are looking at some performance standards.\n",
      "[01:07:40.080 --> 01:07:46.080]  We want something which is given as the best performance gain, but we are only able to estimate it.\n",
      "[01:07:46.080 --> 01:07:52.080]  We are not getting the entire accurate performance gain, we cannot get that.\n",
      "[01:07:52.080 --> 01:07:55.080]  But greedy algorithms can be really bad.\n",
      "[01:07:56.080 --> 01:07:59.080]  Imagine that this is new.\n",
      "[01:07:59.080 --> 01:08:02.080]  You want to now, you are very impatient.\n",
      "[01:08:02.080 --> 01:08:04.080]  You want to reach them early, you are very complete.\n",
      "[01:08:04.080 --> 01:08:07.080]  You see that there is no car over here.\n",
      "[01:08:07.080 --> 01:08:11.080]  You try and take a left.\n",
      "[01:08:11.080 --> 01:08:17.080]  And then because you have only seen this empty spot, you are not able to see these huge tracks.\n",
      "[01:08:17.080 --> 01:08:19.080]  You take a left over here.\n",
      "[01:08:19.080 --> 01:08:23.080]  And then you perpetually caught behind these various global graphs.\n",
      "[01:08:24.080 --> 01:08:26.080]  They are moving at 30 minutes.\n",
      "[01:08:26.080 --> 01:08:30.080]  It would have been better if you have just stayed at this point, right, in the long run.\n",
      "[01:08:30.080 --> 01:08:34.080]  So, what we have done here is to take a very greedy position.\n",
      "[01:08:34.080 --> 01:08:39.080]  We have not considered the global picture or we have not seen very much into the future.\n",
      "[01:08:39.080 --> 01:08:46.080]  We have just seen what is the best I can do right now, which is just take a left and then I will be caught in a slowly.\n",
      "[01:08:46.080 --> 01:08:52.080]  So, this is just to show that greedy is not optimal.\n",
      "[01:08:52.080 --> 01:08:54.080]  Now, we come up with the first algorithm.\n",
      "[01:08:54.080 --> 01:09:02.080]  This particular algorithm is called IDP of some other variations of a decision-free algorithm.\n",
      "[01:09:02.080 --> 01:09:06.080]  We have created three specific arguments.\n",
      "[01:09:06.080 --> 01:09:09.080]  The first is examples target attribute and arguments.\n",
      "[01:09:09.080 --> 01:09:13.080]  Can anyone tell me what you think the target attribute is?\n",
      "[01:09:13.080 --> 01:09:14.080]  Output.\n",
      "[01:09:14.080 --> 01:09:15.080]  Output.\n",
      "[01:09:15.080 --> 01:09:21.080]  So, the output attribute is the response variable or the output variable.\n",
      "[01:09:21.080 --> 01:09:23.080]  Will I create an error somehow?\n",
      "[01:09:23.080 --> 01:09:25.080]  What do you think are the outputs?\n",
      "[01:09:25.080 --> 01:09:33.080]  The features here which were outlook, event, humility, etcetera, are those specific features.\n",
      "[01:09:33.080 --> 01:09:34.080]  What are the examples?\n",
      "[01:09:34.080 --> 01:09:44.080]  Examples are the set of examples are basically the matrix that we have, right, which contains the target attribute and the output.\n",
      "[01:09:44.080 --> 01:09:48.080]  We will start this algorithm by creating root node.\n",
      "[01:09:48.080 --> 01:09:52.080]  In the previous case, we do not know the first cause algorithm.\n",
      "[01:09:52.080 --> 01:09:57.080]  We will first of all this in Sanskrit with an empty root node.\n",
      "[01:09:57.080 --> 01:10:04.080]  So, say that if all the examples are positive or negative or yes or no, then return root to the label class and yes.\n",
      "[01:10:04.080 --> 01:10:06.080]  That does make sense to you.\n",
      "[01:10:06.080 --> 01:10:09.080]  You understand what this means.\n",
      "[01:10:09.080 --> 01:10:17.080]  So, if all of the examples, if all of the examples were yes or no, do we need to create an example?\n",
      "[01:10:17.080 --> 01:10:21.080]  Do we need a decision tree or can we directly or this particular tree or yes or no?\n",
      "[01:10:21.080 --> 01:10:23.080]  We can always predict it to be yes, right.\n",
      "[01:10:23.080 --> 01:10:25.080]  There is no decision involved.\n",
      "[01:10:25.080 --> 01:10:30.080]  There is no specific attribute with a steady many, you are all this little bit.\n",
      "[01:10:30.080 --> 01:10:41.080]  If all the examples, if the attributes is empty, then return root with the most common value of target attribute and examples.\n",
      "[01:10:41.080 --> 01:10:43.080]  We will come to this later.\n",
      "[01:10:43.080 --> 01:10:45.080]  But for now, let us look at the recursive property.\n",
      "[01:10:45.080 --> 01:10:51.080]  This is a very nice intuitive algorithm which can work with the algorithm.\n",
      "[01:10:51.080 --> 01:10:57.080]  You first become an attribute which best classifies the example.\n",
      "[01:10:57.080 --> 01:11:02.080]  Now, if we go back to this tree, we have 50 first attributes as output.\n",
      "[01:11:02.080 --> 01:11:11.080]  So, the inclusion of an outlook will help us get the best estimated performance gain or does the best attribute\n",
      "[01:11:11.080 --> 01:11:17.080]  is the best attribute for classifying the examples.\n",
      "[01:11:17.080 --> 01:11:21.080]  How does the inclusion of best coming, we look at an inventory.\n",
      "[01:11:21.080 --> 01:11:28.080]  Now, the output A was chosen as the attribute output was chosen as A.\n",
      "[01:11:28.080 --> 01:11:32.080]  What are the values that the attribute output could have taken?\n",
      "[01:11:32.080 --> 01:11:37.080]  Sunny overcast or rain.\n",
      "[01:11:37.080 --> 01:11:43.080]  Now, for each of these attributes, can I call the same concept because of any of them?\n",
      "[01:11:43.080 --> 01:11:48.080]  That is the simple inclusion of an algorithm.\n",
      "[01:11:48.080 --> 01:11:51.080]  At each level, you keep on taking the same position.\n",
      "[01:11:51.080 --> 01:11:53.080]  Let us.\n",
      "[01:11:55.080 --> 01:12:06.080]  So, we set the root to be K and for each value of A, it was sunny, sunny, overcast and rainy.\n",
      "[01:12:06.080 --> 01:12:14.080]  You then add new pre-launch and you also reduce your number of examples from now.\n",
      "[01:12:14.080 --> 01:12:21.080]  You restrict the set of examples where the attribute A was the particular value of B.\n",
      "[01:12:21.080 --> 01:12:28.080]  And the examples is empty, you add new to the level of most common value of the target.\n",
      "[01:12:28.080 --> 01:12:32.080]  Otherwise, you call the same procedure.\n",
      "[01:12:32.080 --> 01:12:39.080]  After adding the set of examples on the remaining subset of the query.\n",
      "[01:12:39.080 --> 01:12:46.080]  So, this is how we configure just code of the commit, we will get into the details of it.\n",
      "[01:12:46.080 --> 01:12:53.080]  But before that, we had talked about giving the best estimated performance gain rate.\n",
      "[01:12:53.080 --> 01:12:55.080]  How do we quantify that?\n",
      "[01:12:55.080 --> 01:13:00.080]  So, we come up with a metric of the common with the side-style measure, known as interval.\n",
      "[01:13:00.080 --> 01:13:03.080]  What do you take? Entertain means in general.\n",
      "[01:13:03.080 --> 01:13:06.080]  You would have started from the wrong.\n",
      "[01:13:06.080 --> 01:13:11.080]  Randomness or some impurity in sample.\n",
      "[01:13:11.080 --> 01:13:16.080]  If you see a set like this, can you tell me what is the entropy of this?\n",
      "[01:13:16.080 --> 01:13:23.080]  Randomness or disorder or the amount of unclarity?\n",
      "[01:13:23.080 --> 01:13:30.080]  You know the formula, have you started the formula of entropy?\n",
      "[01:13:30.080 --> 01:13:40.080]  The definition of entropy is you have to find those and 9 a's.\n",
      "[01:13:40.080 --> 01:13:45.080]  Imagine all of these ODEs were yes.\n",
      "[01:13:45.080 --> 01:13:48.080]  We have an disorder in the system.\n",
      "[01:13:48.080 --> 01:13:51.080]  You have anything which is uncertain.\n",
      "[01:13:51.080 --> 01:13:55.080]  So, then in such cases, we will say the entropy is 0.\n",
      "[01:13:55.080 --> 01:14:02.080]  If the answer was 7, yes, and 7 notes, what could you think is the entropy?\n",
      "[01:14:02.080 --> 01:14:04.080]  It would be very high.\n",
      "[01:14:04.080 --> 01:14:09.080]  Because you are very unsure about whether it should be a yes or a no.\n",
      "[01:14:09.080 --> 01:14:17.080]  So, the formula is given in terms of minus p log, the salvation minus p log p for the different classes.\n",
      "[01:14:17.080 --> 01:14:20.080]  We had probability of no as 5 by 14.\n",
      "[01:14:20.080 --> 01:14:23.080]  We had 14 examples, 5 of which were no.\n",
      "[01:14:23.080 --> 01:14:28.080]  And I know that they were yes and log base to probability of no.\n",
      "[01:14:28.080 --> 01:14:32.080]  So, if you do this calculation, it comes out to be 0.9.\n",
      "[01:14:32.080 --> 01:14:34.080]  This is the daily 9.\n",
      "[01:14:34.080 --> 01:14:37.080]  We also get a curve by this.\n",
      "[01:14:37.080 --> 01:14:40.080]  So, probability of yes was 0.\n",
      "[01:14:40.080 --> 01:14:42.080]  The entropy is 0.\n",
      "[01:14:42.080 --> 01:14:46.080]  This means that there is no disorder in all the examples negative.\n",
      "[01:14:46.080 --> 01:14:53.080]  And if you look at the other extreme of all the probability of class or yes is 1, there is a gain or entropy\n",
      "[01:14:53.080 --> 01:14:55.080]  because there is no uncertainty.\n",
      "[01:14:55.080 --> 01:15:02.080]  The maximum uncertainty or ability happens when the probability of class is the same as probability of n.\n",
      "[01:15:02.080 --> 01:15:08.080]  So, we will start up with calculating an entropy of a set.\n",
      "[01:15:08.080 --> 01:15:15.080]  And we now want to use an attitude A, which is able to give us the biggest performance gain.\n",
      "[01:15:15.080 --> 01:15:20.080]  So, can you think of in terms of starting with entropy as a starting point?\n",
      "[01:15:20.080 --> 01:15:27.080]  What manipulation or what are the statistical measures we need to tell this is the best attribute?\n",
      "[01:15:27.080 --> 01:15:30.080]  Please get it.\n",
      "[01:15:30.080 --> 01:15:32.080]  Please get it.\n",
      "[01:15:32.080 --> 01:15:34.080]  One is the objective.\n",
      "[01:15:34.080 --> 01:15:39.080]  Okay, but how do you calculate the entropy of an entropy?\n",
      "[01:15:40.080 --> 01:15:44.080]  For me, it is a paper bit in terms of we have to choose an attribute.\n",
      "[01:15:44.080 --> 01:15:51.080]  So, before before it started off with the elementary learning, you had several examples.\n",
      "[01:15:51.080 --> 01:15:54.080]  You can calculate the entropy of that sample.\n",
      "[01:15:54.080 --> 01:15:58.080]  Now, you want to choose an attribute which will lower the entropy.\n",
      "[01:15:58.080 --> 01:15:59.080]  Right?\n",
      "[01:15:59.080 --> 01:16:03.080]  So, basically if we go at this point.\n",
      "[01:16:03.080 --> 01:16:07.080]  So, we said that there was a lot of uncertainty in whether I will get it from now.\n",
      "[01:16:07.080 --> 01:16:08.080]  Right?\n",
      "[01:16:08.080 --> 01:16:10.080]  Which means that there is a lot of uncertainty.\n",
      "[01:16:10.080 --> 01:16:16.080]  But if I choose output as overclass, I know that I will get it at T later.\n",
      "[01:16:16.080 --> 01:16:17.080]  Right?\n",
      "[01:16:17.080 --> 01:16:21.080]  So, there is no element of entropy involved there for those specific examples.\n",
      "[01:16:21.080 --> 01:16:22.080]  Right?\n",
      "[01:16:22.080 --> 01:16:29.080]  So, we will now see that we are trying to choose an attribute subject to choosing in which the\n",
      "[01:16:29.080 --> 01:16:31.080]  entropy becomes lower.\n",
      "[01:16:31.080 --> 01:16:32.080]  Right?\n",
      "[01:16:32.080 --> 01:16:34.080]  There is a lesser disorder in this system.\n",
      "[01:16:35.080 --> 01:16:39.080]  So, that concept is known as information gain.\n",
      "[01:16:39.080 --> 01:16:42.080]  I just look at the, I just show the formula.\n",
      "[01:16:48.080 --> 01:16:55.080]  The information gain is known as the formula is given in terms of the reaction entropy.\n",
      "[01:16:55.080 --> 01:16:59.080]  By partitioning a set of examples, S on an attribute gain.\n",
      "[01:16:59.080 --> 01:17:01.080]  So, an attribute gain would have taken different values.\n",
      "[01:17:01.080 --> 01:17:06.080]  For example, the output wrapping is sunny, rainy or overclassed.\n",
      "[01:17:06.080 --> 01:17:14.080]  We then write the gain on a set of examples S subject to an attribute A as defined by\n",
      "[01:17:14.080 --> 01:17:16.080]  the entropy which is the initial system.\n",
      "[01:17:16.080 --> 01:17:23.080]  Of all the examples, minus the weighted entropy is weighted by the number of samples you have\n",
      "[01:17:23.080 --> 01:17:24.080]  for a particular value.\n",
      "[01:17:24.080 --> 01:17:29.080]  But in fact, with the entropy of the subsets that we have written.\n",
      "[01:17:29.080 --> 01:17:30.080]  Right?\n",
      "[01:17:31.080 --> 01:17:37.080]  I will not go into the details but now let us assume that at this point of time we have\n",
      "[01:17:37.080 --> 01:17:38.080]  four days examples.\n",
      "[01:17:38.080 --> 01:17:40.080]  We choose an output as the first node.\n",
      "[01:17:40.080 --> 01:17:42.080]  The entropy of this set is zero.\n",
      "[01:17:42.080 --> 01:17:43.080]  Right?\n",
      "[01:17:43.080 --> 01:17:44.080]  The weighted entropy of this set is also zero.\n",
      "[01:17:44.080 --> 01:17:46.080]  This side will have some weighted entropy.\n",
      "[01:17:46.080 --> 01:17:50.080]  So, let us stop at this point with the input.\n",
      "[01:17:50.080 --> 01:17:55.080]  We are trying to choose an attribute which reduces the entropy.\n",
      "[01:17:55.080 --> 01:17:59.080]  Let us see it at 11am on our target.\n",
      "[01:17:59.080 --> 01:18:04.080]  Thank you.\n"
     ]
    }
   ],
   "source": [
    "transcription = whisper_model.transcribe(\"audio/CuBzyh4Xmvk.m4a\", fp16=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f6322b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'segments', 'language'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcription.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b106d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_srt_from_transcription(transcription_objects, srt_file_path):\n",
    "    with open(srt_file_path, 'w') as srt_file:\n",
    "        index = 1  # SRT format starts with index 1\n",
    "\n",
    "        for entry in transcription_objects['segments']:\n",
    "            start_time = entry['start']\n",
    "            end_time = entry['end']\n",
    "            text = entry['text']\n",
    "\n",
    "            # Convert time to SRT format\n",
    "            start_time_str = format_time(start_time)\n",
    "            end_time_str = format_time(end_time)\n",
    "\n",
    "            # Write entry to SRT file\n",
    "            srt_file.write(f\"{index}\\n\")\n",
    "            srt_file.write(f\"{start_time_str} --> {end_time_str}\\n\")\n",
    "            srt_file.write(f\"{text}\\n\\n\")\n",
    "\n",
    "            index += 1\n",
    "\n",
    "def format_time(time_seconds):\n",
    "    minutes, seconds = divmod(time_seconds, 60)\n",
    "    hours, minutes = divmod(minutes, 60)\n",
    "    return f\"{int(hours):02d}:{int(minutes):02d}:{int(seconds):02d},000\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0dd53e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_srt_from_transcription(transcription, \"audio/CuBzyh4Xmvk.srt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b592eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "00:00:00,000 --> 00:00:05,000\n",
      " Please look at the code mentioned above and please sign up on the Google Cloud.\n",
      "\n",
      "2\n",
      "00:00:05,000 --> 00:00:08,000\n",
      " We've already started making some announcements.\n",
      "\n",
      "3\n",
      "00:00:08,000 --> 00:00:14,000\n"
     ]
    }
   ],
   "source": [
    "!head audio/CuBzyh4Xmvk.srt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41c79ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler \n",
    "                                 \n",
    "llm = Ollama(model=\"llama2\", \n",
    "             callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "179ea1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide a bullet-point summary for the given text:\n",
      "\n",
      "The lecture discusses the concept of entropy and its relationship to decision trees, particularly in the context of information gain. The speaker explains that entropy is a measure of disorder or uncertainty in a system, and that in decision tree learning, we want to choose an attribute that reduces the entropy of the system. This attribute is called the information gain, and it is calculated by partitioning the set of examples based on an attribute and then weighting the entropy of each subset. The speaker also mentions that the weighted entropy is zero for the entire set of examples, but the side with the highest entropy has the most information gain.\n",
      "\n",
      "The lecture starts by explaining that decision trees are constructed using a greedy algorithm, which chooses the attribute that gives the biggest estimated performance gain at each level of the tree. However, this greedy approach is not optimal, as it does not consider the global picture or the future consequences of the choices made. To address this limitation, we use the information gain concept to guide the decision-making process and choose an attribute that reduces the entropy of the system.\n",
      "\n",
      "The speaker then provides examples to illustrate the idea of entropy and information gain. In one example, a set of 14 examples is given, with 5 of them labeled as \"no\" and the rest labeled as \"yes\". The entropy of this set is calculated using the formula log2(p) = log2(0.5) = 1. The speaker then shows how to calculate the information gain for each attribute, which is the reduction in entropy that can be achieved by partitioning the set based on that attribute.\n",
      "\n",
      "Overall, the lecture provides a deeper understanding of the decision tree learning algorithm and how it incorporates the concept of entropy to create the most informative trees possible.The lecture discusses the concept of entropy and its relationship to decision trees, particularly in the context of information gain. The speaker explains that entropy is a measure of disorder or uncertainty in a system, and that in decision tree learning, we want to choose an attribute that reduces the entropy of the system. This attribute is called the information gain, and it is calculated by partitioning the set of examples based on an attribute and then weighting the entropy of each subset. The speaker also mentions that the weighted entropy is zero for the entire set of examples, but the side with the highest entropy has the most information gain.\n",
      "\n",
      "The lecture starts by explaining that decision trees are constructed using a greedy algorithm, which chooses the attribute that gives the biggest estimated performance gain at each level of the tree. However, this greedy approach is not optimal, as it does not consider the global picture or the future consequences of the choices made. To address this limitation, we use the information gain concept to guide the decision-making process and choose an attribute that reduces the entropy of the system.\n",
      "\n",
      "The speaker then provides examples to illustrate the idea of entropy and information gain. In one example, a set of 14 examples is given, with 5 of them labeled as \"no\" and the rest labeled as \"yes\". The entropy of this set is calculated using the formula log2(p) = log2(0.5) = 1. The speaker then shows how to calculate the information gain for each attribute, which is the reduction in entropy that can be achieved by partitioning the set based on that attribute.\n",
      "\n",
      "Overall, the lecture provides a deeper understanding of the decision tree learning algorithm and how it incorporates the concept of entropy to create the most informative trees possible.\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Summarize the following in Markdown bullets:\n",
      "\n",
      "The lecture notes are discussing the concept of decision trees and how they are used in machine learning to classify data. The main idea is to use a greedy algorithm to construct a decision tree that maximizes the expected performance gain. However, the greedy algorithm is not optimal as it does not consider the global picture or the future consequences of each decision made.\n",
      "\n",
      "The notes explain how to calculate the entropy of a set and how it can be used to choose an attribute that reduces the entropy, which is known as information gain. The formula for calculating the entropy of a set is given as - p log p, where p is the probability of belonging to a particular class. The notes also explain how to calculate the weighted entropy of a subset of examples and how to use this concept to choose an attribute that reduces the entropy.\n",
      "\n",
      "The lecture notes cover the following topics:\n",
      "\n",
      "1. Decision trees and their construction using a greedy algorithm.\n",
      "2. The concept of entropy and how it can be used to choose an attribute that reduces the entropy.\n",
      "3. The formula for calculating the entropy of a set and the weighted entropy of a subset of examples.\n",
      "4. How to use information gain to choose an attribute that reduces the entropy.\n",
      "\n",
      "Overall, the lecture notes provide a good introduction to decision trees and their construction, as well as the concept of entropy and how it can be used to improve the performance of machine learning algorithms.The lecture notes are discussing the concept of decision trees and how they are used in machine learning to classify data. The main idea is to use a greedy algorithm to construct a decision tree that maximizes the expected performance gain. However, the greedy algorithm is not optimal as it does not consider the global picture or the future consequences of each decision made.\n",
      "\n",
      "The notes explain how to calculate the entropy of a set and how it can be used to choose an attribute that reduces the entropy, which is known as information gain. The formula for calculating the entropy of a set is given as - p log p, where p is the probability of belonging to a particular class. The notes also explain how to calculate the weighted entropy of a subset of examples and how to use this concept to choose an attribute that reduces the entropy.\n",
      "\n",
      "The lecture notes cover the following topics:\n",
      "\n",
      "1. Decision trees and their construction using a greedy algorithm.\n",
      "2. The concept of entropy and how it can be used to choose an attribute that reduces the entropy.\n",
      "3. The formula for calculating the entropy of a set and the weighted entropy of a subset of examples.\n",
      "4. How to use information gain to choose an attribute that reduces the entropy.\n",
      "\n",
      "Overall, the lecture notes provide a good introduction to decision trees and their construction, as well as the concept of entropy and how it can be used to improve the performance of machine learning algorithms.\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Highlight the important topics and subtopics in the given lecture:\n",
      "\n",
      "\n",
      "The speaker is discussing the concept of entropy and how it relates to decision trees in machine learning. Entropy is a measure of the amount of disorder or uncertainty in a system, and in the context of decision trees, it can be used to quantify the amount of information gained by choosing a particular attribute at each node.\n",
      "\n",
      "The speaker explains that the entropy of a set of examples can be calculated using the formula log(p(x))/log(p(y)), where x is the current set of examples and y is the target variable. The higher the entropy, the more uncertain the system is about the target variable, and the more information gain can be achieved by choosing an attribute that reduces the entropy.\n",
      "\n",
      "The speaker then introduces the concept of information gain, which is the decrease in entropy after choosing an attribute at each node. The information gain is calculated as the weighted entropy of the subset of examples that have been assigned to a particular value of the chosen attribute, minus the initial entropy of the entire set of examples.\n",
      "\n",
      "The speaker then provides an example of how to calculate the entropy and information gain of a set of examples using a decision tree. In this example, the speaker assumes that there are four days of data and chooses the output attribute as the first node. The entropy of this set is zero, and the weighted entropy of this set is also zero. The speaker then explains how to calculate the information gain for each possible value of the chosen attribute.\n",
      "\n",
      "Overall, the speaker's explanation of entropy and information gain in the context of decision trees provides a useful framework for understanding the role of these concepts in machine learning. By quantifying the amount of disorder or uncertainty in a system, entropy can help guide the selection of attributes at each node, leading to more accurate predictions and better decision-making.\n",
      "The speaker is discussing the concept of entropy and how it relates to decision trees in machine learning. Entropy is a measure of the amount of disorder or uncertainty in a system, and in the context of decision trees, it can be used to quantify the amount of information gained by choosing a particular attribute at each node.\n",
      "\n",
      "The speaker explains that the entropy of a set of examples can be calculated using the formula log(p(x))/log(p(y)), where x is the current set of examples and y is the target variable. The higher the entropy, the more uncertain the system is about the target variable, and the more information gain can be achieved by choosing an attribute that reduces the entropy.\n",
      "\n",
      "The speaker then introduces the concept of information gain, which is the decrease in entropy after choosing an attribute at each node. The information gain is calculated as the weighted entropy of the subset of examples that have been assigned to a particular value of the chosen attribute, minus the initial entropy of the entire set of examples.\n",
      "\n",
      "The speaker then provides an example of how to calculate the entropy and information gain of a set of examples using a decision tree. In this example, the speaker assumes that there are four days of data and chooses the output attribute as the first node. The entropy of this set is zero, and the weighted entropy of this set is also zero. The speaker then explains how to calculate the information gain for each possible value of the chosen attribute.\n",
      "\n",
      "Overall, the speaker's explanation of entropy and information gain in the context of decision trees provides a useful framework for understanding the role of these concepts in machine learning. By quantifying the amount of disorder or uncertainty in a system, entropy can help guide the selection of attributes at each node, leading to more accurate predictions and better decision-making.\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Give us some question for a quiz based on the following text:\n",
      "\n",
      "The lecture notes discuss the concept of entropy and its relationship to decision trees, specifically in the context of information gain. The notes explain that entropy is a measure of the disorder or uncertainty in a system, and that the goal of decision tree learning is to find an attribute that reduces the entropy of the system. This attribute is called the information gain.\n",
      "\n",
      "The lecture notes also introduce the idea of weighted entropy, which takes into account the number of examples in each subset of the data. The notes explain that by choosing an attribute that reduces the weighted entropy of the system, we can create a decision tree that is more accurate and informative.\n",
      "\n",
      "Overall, the lecture notes provide a useful framework for understanding the role of entropy in decision tree learning and the importance of choosing attributes that reduce uncertainty in the system.The lecture notes discuss the concept of entropy and its relationship to decision trees, specifically in the context of information gain. The notes explain that entropy is a measure of the disorder or uncertainty in a system, and that the goal of decision tree learning is to find an attribute that reduces the entropy of the system. This attribute is called the information gain.\n",
      "\n",
      "The lecture notes also introduce the idea of weighted entropy, which takes into account the number of examples in each subset of the data. The notes explain that by choosing an attribute that reduces the weighted entropy of the system, we can create a decision tree that is more accurate and informative.\n",
      "\n",
      "Overall, the lecture notes provide a useful framework for understanding the role of entropy in decision tree learning and the importance of choosing attributes that reduce uncertainty in the system.\n",
      "\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_qs = [\"Please provide a bullet-point summary for the given text:\",\n",
    "             \"Summarize the following in Markdown bullets:\",\n",
    "             \"Highlight the important topics and subtopics in the given lecture:\",\n",
    "             \"Give us some question for a quiz based on the following text:\"]\n",
    "\n",
    "prompts = [q + \"\\n\" + transcription[\"text\"] for q in prompt_qs]\n",
    "\n",
    "for prompt, prompt_qs in zip(prompts, prompt_qs):\n",
    "    print(prompt_qs, end=\"\\n\\n\")\n",
    "    output = llm(prompt)\n",
    "    print(output, end=\"\\n\\n\")\n",
    "    print(\"==\"*50, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b39c75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
